{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dc6265d-b51d-45a2-b3b5-ae6da76dfbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/dlkj48z90j97v_lywzxm0kh00000gr/T/ipykernel_49849/800166419.py:7: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  Geneious_raw_DF=pd.read_csv(\"Annotations.csv\", engine='python', error_bad_lines=False)                                                    ##Import raw Geneious output table for variant analysis genes. Set up engine equals python and error_bad_lines for preventing errors with processing large csv files\n",
      "/var/folders/63/dlkj48z90j97v_lywzxm0kh00000gr/T/ipykernel_49849/800166419.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Geneious_DF_filtered_poly[\"TrackerSNP\"]=(Geneious_DF_filtered_poly[\"Amino Acid Change\"].astype(str).str[0]+                               ##Create a TrackerSNP Column which has both amino acid before the change\n",
      "/var/folders/63/dlkj48z90j97v_lywzxm0kh00000gr/T/ipykernel_49849/800166419.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered_dup[[\"SITE\",'TREATMENT_DAY', \"POOLED\",\"YEAR\",\"TYPE\",\"SNP\",\"Document Name\" ,\"POOLEDSIZE\"]] = Combination_filtered_dup.apply([site,TreatmentDay,POOLED,year,type_SNP,SNP,name,POOLEDsize], axis=1)\n",
      "/var/folders/63/dlkj48z90j97v_lywzxm0kh00000gr/T/ipykernel_49849/800166419.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered_dup[[\"SITE\",'TREATMENT_DAY', \"POOLED\",\"YEAR\",\"TYPE\",\"SNP\",\"Document Name\" ,\"POOLEDSIZE\"]] = Combination_filtered_dup.apply([site,TreatmentDay,POOLED,year,type_SNP,SNP,name,POOLEDsize], axis=1)\n",
      "/var/folders/63/dlkj48z90j97v_lywzxm0kh00000gr/T/ipykernel_49849/800166419.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered_dup[[\"SITE\",'TREATMENT_DAY', \"POOLED\",\"YEAR\",\"TYPE\",\"SNP\",\"Document Name\" ,\"POOLEDSIZE\"]] = Combination_filtered_dup.apply([site,TreatmentDay,POOLED,year,type_SNP,SNP,name,POOLEDsize], axis=1)\n",
      "/var/folders/63/dlkj48z90j97v_lywzxm0kh00000gr/T/ipykernel_49849/800166419.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered_dup[[\"SITE\",'TREATMENT_DAY', \"POOLED\",\"YEAR\",\"TYPE\",\"SNP\",\"Document Name\" ,\"POOLEDSIZE\"]] = Combination_filtered_dup.apply([site,TreatmentDay,POOLED,year,type_SNP,SNP,name,POOLEDsize], axis=1)\n",
      "/var/folders/63/dlkj48z90j97v_lywzxm0kh00000gr/T/ipykernel_49849/800166419.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered_dup[[\"SITE\",'TREATMENT_DAY', \"POOLED\",\"YEAR\",\"TYPE\",\"SNP\",\"Document Name\" ,\"POOLEDSIZE\"]] = Combination_filtered_dup.apply([site,TreatmentDay,POOLED,year,type_SNP,SNP,name,POOLEDsize], axis=1)\n",
      "/var/folders/63/dlkj48z90j97v_lywzxm0kh00000gr/T/ipykernel_49849/800166419.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered_dup[[\"SITE\",'TREATMENT_DAY', \"POOLED\",\"YEAR\",\"TYPE\",\"SNP\",\"Document Name\" ,\"POOLEDSIZE\"]] = Combination_filtered_dup.apply([site,TreatmentDay,POOLED,year,type_SNP,SNP,name,POOLEDsize], axis=1)\n",
      "/var/folders/63/dlkj48z90j97v_lywzxm0kh00000gr/T/ipykernel_49849/800166419.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered_dup[[\"SITE\",'TREATMENT_DAY', \"POOLED\",\"YEAR\",\"TYPE\",\"SNP\",\"Document Name\" ,\"POOLEDSIZE\"]] = Combination_filtered_dup.apply([site,TreatmentDay,POOLED,year,type_SNP,SNP,name,POOLEDsize], axis=1)\n",
      "/var/folders/63/dlkj48z90j97v_lywzxm0kh00000gr/T/ipykernel_49849/800166419.py:96: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  final_combine_4=final_combine_4.drop('POOLEDSIZE', 1)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'voinew3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 102>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m#final_combine_4=final_combine_4[final_combine_4['SITE'].notnull()] ##Check if the site is empty or not and assign site \u001b[39;00m\n\u001b[1;32m    100\u001b[0m final_combine_4\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mET_individual_EPI.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m##Create a file with the dataframe for testing purpose this is internal lab version\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m df_voi\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvoinew3.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m df_voi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVOI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mdf_voi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefAA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m+\u001b[39mdf_voi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAPos\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m+\u001b[39mdf_voi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAltAA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    104\u001b[0m voi_list\u001b[38;5;241m=\u001b[39mdf_voi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVOI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'voinew3.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd           ## Import Pandas library for processing dataframe as pd\n",
    "import numpy as np            ## Import Numy for processing matrix as np\n",
    "import sys                    ##Import sys library for maximizing the csv limit for large csv file which can be Geneious file.\n",
    "import csv                    ##Import csv module to input the csv file to dataframe \n",
    "\n",
    "csv.field_size_limit(sys.maxsize)                                                                                                         ##Maximize the csv file input size\n",
    "Geneious_raw_DF=pd.read_csv(\"Annotations.csv\", engine='python', error_bad_lines=False)                                                    ##Import raw Geneious output table for variant analysis genes. Set up engine equals python and error_bad_lines for preventing errors with processing large csv files\n",
    "Geneious_DF_filtered_poly=Geneious_raw_DF[(Geneious_raw_DF['Type']=='Polymorphism') & (Geneious_raw_DF['Amino Acid Change'].notnull())]   ##If the type column contains polymorphism and Amino Acid Change column is not empty then create a dataframe satisfying those conditions\n",
    "Geneious_DF_filtered_cov=Geneious_raw_DF[Geneious_raw_DF['Type']=='Coverage - High']                                                      ##If the Coverage - High is in the type column as value then select dataframe for those column values\n",
    "Geneious_DF_filtered_poly[\"TrackerSNP\"]=(Geneious_DF_filtered_poly[\"Amino Acid Change\"].astype(str).str[0]+                               ##Create a TrackerSNP Column which has both amino acid before the change\n",
    "Geneious_DF_filtered_poly[\"CDS Codon Number\"].astype(int).astype(str)+                                                                    ##Create a TrackerSNP Column which has CDC Codon number for amino acid change location\n",
    "Geneious_DF_filtered_poly[\"Amino Acid Change\"].astype(str).str[-1])                                                                       ##Create a TrackerSNP Column which has both amino acid after the change\n",
    "Pre_Combined_Variant_Wildtype = [Geneious_DF_filtered_poly, Geneious_DF_filtered_cov]                                                     ##Produce a complete dataframe which contains both variants and wildtypes\n",
    "Combined_Vi_Wi = pd.concat(Pre_Combined_Variant_Wildtype)                                                                                 ##Concatenate the dataframes for variants and wildtypes\n",
    "Combination_filtered_dup=Combined_Vi_Wi.drop_duplicates(subset =[\"Document Name\", \"TrackerSNP\"] )                                         ##Drop duplicates meaning if the values are already in variants then drop it from the wildtypes\n",
    "                                                                                                                                          ##The dataframe contains information, \"Sample,POOLED,Year,SITE,TreatmentDay,GENE,G_ANNOTATION,COVERAGE,VAF,VF,SNP,TYPE\\n\")\n",
    "\n",
    "def site(row):                                ##Set up a function for assignging site based on the values in the document name column. This will change with each study please update the sites. \n",
    "    if row['Document Name'][4:6]==\"Am\": ##Assign Zaire for the abbreviation\n",
    "        return 'Amhara'\n",
    "\n",
    "\n",
    "def POOLEDsize(row):                                          ##Set up a function for assigning TreatmentDay based on the values in the document name column\n",
    "    if row['Document Name'].replace(\" \",\"\")[13:14]==\"0\":      ##If 0 in the position 13 of the string\n",
    "        return row['Document Name'].replace(\" \",\"\")[14:15]    ##then assign only position 14 as POOLED size\n",
    "    elif row['Document Name'].replace(\" \",\"\")[13:14]!=\"0\":    ##If 0 not in the position 13 of the string\n",
    "        return row['Document Name'].replace(\" \",\"\")[13:15]    ##then assign the string from 13 and 14\n",
    "\n",
    "def TreatmentDay(row): ##Set up a function for assigning TreatmentDay based on the values in the document name column\n",
    "    if row['Document Name'][6:8]==\"00\":\n",
    "        return '0'\n",
    "    elif row['Document Name'][6:8]==\"1A\": ##To do: confirm this with Stefano whether it was a naming issue. There was also 1P which caused one more problem\n",
    "        return '1'\n",
    "    elif row['Document Name'][6:8]!=\"00\" and row['Document Name'][6:8]!=\"1A\":\n",
    "        return row['Document Name'][6:8]   \n",
    "\n",
    "def POOLED(row):                                              ##Set up a function for POOLED based on the values in the document name column\n",
    "    if row['Document Name'].replace(\" \",\"\")[12:13]==\"p\":      ##If position 12 has p in it\n",
    "        return 'POOLED'                                       ##then it is considered POOLED sample\n",
    "    elif row['Document Name'].replace(\" \",\"\")[12:13]!=\"p\":    ##If position 12 has no p in it\n",
    "        return 'individual'                                   ##then it is considered individual sample\n",
    "\n",
    "def year(row):  ##Set up a function for Year  based on the values in the document name column\n",
    "    return row['Document Name'][0:2]\n",
    "\n",
    "def type_SNP(row):  ##Set up a TYPE column based on given value in the Type whether it is mutation or wildtype\n",
    "    if row['Type'] =='Polymorphism':  ##Type is either mutation or wildtype\n",
    "        return \"mutation\"\n",
    "    if row['Type'] =='Coverage - High':\n",
    "        return \"wildtype\"\n",
    "    \n",
    "def SNP(row):  ##Set up a SNP column to give pre or post amino acid changes based on mutation or wildtype\n",
    "    if row['Type'] =='Polymorphism':  ##Set up SNP notation for wildtype or mutation\n",
    "        return row['TrackerSNP'][1::]\n",
    "    if row['Type'] =='Coverage - High':\n",
    "        return row['TrackerSNP'][0:-1]\n",
    "    \n",
    "# Dhruvi: added this below to split the document name and now in step2, no error occurs due to reruning the code.\n",
    "def name(row):\n",
    "    return row['Document Name'].split(\"_\")[0].replace(\" \",\"\") ##Split the document name with _ and assign first substring to clean up the name of the sample\n",
    "  \n",
    "#Dhruvi : ##Apply the functions defined previously for assigning site,TreatmentDay,POOLED,year,type,SNP,name\n",
    "Combination_filtered_dup[[\"SITE\",'TREATMENT_DAY', \"POOLED\",\"YEAR\",\"TYPE\",\"SNP\",\"Document Name\" ,\"POOLEDSIZE\"]] = Combination_filtered_dup.apply([site,TreatmentDay,POOLED,year,type_SNP,SNP,name,POOLEDsize], axis=1)\n",
    "\n",
    "Combination_report1=Combination_filtered_dup[Combination_filtered_dup['Type']=='Polymorphism'] ##Select columns with mutations\n",
    "Combination_report2=Combination_filtered_dup[Combination_filtered_dup['Type']=='Coverage - High'] ##Select columns with wildtypes\n",
    "final_report1=Combination_report1[[\"Document Name\",\"Sequence Name\",\"SITE\",'TREATMENT_DAY',\"POOLED\",\"YEAR\",\"Coverage\",\"Variant Frequency\",\"Variant Raw Frequency\",\"TrackerSNP\",\"TYPE\",\"SNP\",\"POOLEDSIZE\"]] ##Assign sample information to samples with mutation\n",
    "final_report2=Combination_report2[[\"Document Name\",\"Sequence Name\",\"SITE\",'TREATMENT_DAY',\"POOLED\",\"YEAR\",\"Average Coverage\",\"Variant Frequency\",\"Variant Raw Frequency\",\"TrackerSNP\",\"TYPE\",\"SNP\",\"POOLEDSIZE\"]] ##Assign sample information to samples with wildtypes\n",
    "final_report2_re=final_report2.rename(columns={'Average Coverage': 'Coverage'}) ##Change the name of average coverage to coverage for samples with wildtypes\n",
    "final_combine=[final_report1, final_report2_re] ##Combine the information from wildtypes and mutations into one dataframe\n",
    "final_combine_2=pd.concat(final_combine) ##concatenate\n",
    "#print(final_combine_2)\n",
    "#print(final_combine_4)\n",
    "###This part is for polishing lab version of individual\n",
    "final_combine_4=final_combine_2[final_combine_2[\"POOLED\"]==\"individual\"] ##In order to create table just for individual for Matt's report \n",
    "#print(final_combine_4)\n",
    "##create another dataframe where POOLED column is just individual\n",
    "#Dhruvi : replace the rename part with one line code using dictonary\n",
    "final_combine_4=final_combine_4.rename(columns={'Document Name': 'ET_ID_IND','Sequence Name': 'GENE','TrackerSNP': 'G_ANNOTATION', 'Coverage': 'COVERAGE','Variant Frequency': 'VAF','Variant Raw Frequency': 'VF' })\n",
    "\n",
    "final_combine_4=final_combine_4.replace({'DHPS_437Corrected ': 'DHPS'})  ##Rename DHPS_437Corrected values to DHPS to correct the name\n",
    "\n",
    "#Dhruvi : we can remove this part as sample name is already splited at begining using name function.\n",
    "\n",
    "\n",
    "cols = list(final_combine_4.columns) ##Change columns into list\n",
    "a, b = cols.index('SNP'), cols.index('TYPE') ##Index the column to swtich the order to match the previous Guinea report\n",
    "cols[b], cols[a] = cols[a], cols[b] ##Change order of the column\n",
    "final_combine_4 = final_combine_4[cols] ##After reordering the columns reassign to the dataframe\n",
    "# Dhruvi: fill the empty value on multiple columns using dict\n",
    "values = {\"VAF\": \"0%\", \"VF\": \"0\"}\n",
    "final_combine_4 = final_combine_4.fillna(value=values)\n",
    "\n",
    "final_combine_4=final_combine_4.replace({'Pf': 1})\n",
    "final_combine_4=final_combine_4.replace({'xx': 1})\n",
    "final_combine_4=final_combine_4.drop('POOLEDSIZE', 1)\n",
    "final_combine_4=final_combine_4.replace({'DHPS ': 'DHPS'})\n",
    "\n",
    "#final_combine_4=final_combine_4[final_combine_4['SITE'].notnull()] ##Check if the site is empty or not and assign site \n",
    "final_combine_4.to_csv(\"ET_individual_EPI.csv\", sep=',', index=False) ##Create a file with the dataframe for testing purpose this is internal lab version\n",
    "\n",
    "df_voi=pd.read_csv(\"voinew3.csv\")\n",
    "df_voi[\"VOI\"]=df_voi[\"RefAA\"]+df_voi[\"AAPos\"].astype(str)+df_voi[\"AltAA\"]\n",
    "voi_list=df_voi[\"VOI\"].tolist()\n",
    "\n",
    "final_combine_4_voi=final_combine_4[final_combine_4.G_ANNOTATION.isin(voi_list)]\n",
    "#print(final_combine_4_voi)\n",
    "final_combine_4_voi.to_csv(\"ET_individual_EPI_VOI.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfb1ef0-8b1c-41ea-9909-9719ff52e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POOLED_part1=pd.read_csv(\"POOLED_Info.csv\") ##import a file with information about POOLED samples\n",
    "\n",
    "#final_combine_2.rename(columns={'Document Name':'AMD ID (POOLED)'}, inplace=True) ##Reanme the document name to AET_ID\n",
    "\n",
    "\n",
    "df_merged_poolsize = final_combine_2#[final_combine_2[\"POOLED\"]==\"POOLED\"] ##merge the information based on AET_ID to add POOLED columns and POOLED size columns\n",
    "\n",
    "#df_merged_poolsize = df_merged_poolsize.drop(['SITE ','Year_y'], axis=1) ##Get rid of duplicate columns which is SITE and year_y\n",
    "\n",
    "#df_merged_poolsize.PoolSize.fillna(value=1, inplace=True) ##Fill empty values for poolsize for individual with 1s\n",
    "\n",
    "df_merged_poolsize['Variant Frequency'] = df_merged_poolsize['Variant Frequency'].fillna(\"0%\") ##Fill the empty Variant Frequency value with 0\n",
    "df_merged_poolsize['Variant Raw Frequency'] = df_merged_poolsize['Variant Raw Frequency'].fillna(0) ##Fill empty values with 0s\n",
    "#print(df_merged_poolsize)\n",
    "#df_merged_poolsize[\"Prod\"]=df_merged_poolsize[\"Variant Frequency\"].str.split('%').str[0].str.strip(\"%\").astype(float)*df_merged_poolsize[\"POOLEDsize\"].astype(float) ##Get a product by multiplying variant frequency times the pool size\n",
    "df_merged_poolsize.rename(columns={'Variant Frequency':'VAF', 'Variant Raw Frequency':'VF'}, inplace=True) ##Reanme the document name to AET_ID\n",
    "df_merged_poolsize['Coverage'] = df_merged_poolsize['Coverage'].apply(str) ##Change values from numeric to string\n",
    "df_merged_poolsize['VAF'] = df_merged_poolsize['VAF'].apply(str) ##Change values from numeric to string\n",
    "df_merged_poolsize['VF'] = df_merged_poolsize['VF'].apply(str)  ##Change values from numeric to string\n",
    "df_merged_poolsize.Coverage= df_merged_poolsize.Coverage.str.split(\" \").str[0]  ##Split the string by space for values with -> then pick the substring before the first space\n",
    "df_merged_poolsize.VAF= df_merged_poolsize.VAF.str.split(\" \").str[0] ##Split the string by space for values with -> then pick the substring before the first space\n",
    "df_merged_poolsize.VF= df_merged_poolsize.VF.str.split(\" \").str[0] ##Split the string by space for values with -> then pick the substring before the first space\n",
    "df_merged_poolsize.rename(columns={'VAF': 'Variant Frequency', 'VF':'Variant Raw Frequency'}, inplace=True) ##Reanme the document name to AET_ID\n",
    "df_merged_poolsize=df_merged_poolsize.rename(columns={'Document Name': 'ET_ID_POOLED','Sequence Name': 'GENE','TrackerSNP': 'G_ANNOTATION', 'Coverage': 'COVERAGE','Variant Frequency': 'VAF','Variant Raw Frequency': 'VF'  })\n",
    "\n",
    "\n",
    "df_merged_poolsize=df_merged_poolsize.replace({'DHPS_437Corrected ': 'DHPS'})  ##Rename DHPS_437Corrected values to DHPS to correct the name\n",
    "df_merged_poolsize=df_merged_poolsize.replace({'DHPS ': 'DHPS'}) \n",
    "df_merged_poolsize=df_merged_poolsize.replace({'Pf': 1})\n",
    "\n",
    "df_merged_poolsize2=df_merged_poolsize[df_merged_poolsize[\"POOLED\"]==\"POOLED\"]\n",
    "#df_merged_poolsize2 = df_merged_poolsize2.drop('Prod',  1)\n",
    "\n",
    "df_merged_poolsize2.to_csv(\"ET_POOLED_EPI.csv\", index=False)\n",
    "\n",
    "df_merged_poolsize_ind_pool=df_merged_poolsize\n",
    "#df_merged_poolsize2 = df_merged_poolsize2.drop('Prod',  1)\n",
    "\n",
    "df_voi=pd.read_csv(\"voinew3.csv\")\n",
    "df_voi[\"VOI\"]=df_voi[\"RefAA\"]+df_voi[\"AAPos\"].astype(str)+df_voi[\"AltAA\"]\n",
    "voi_list=df_voi[\"VOI\"].tolist()\n",
    "\n",
    "df_merged_poolsize2_voi=df_merged_poolsize2[df_merged_poolsize2.G_ANNOTATION.isin(voi_list)]\n",
    "\n",
    "df_merged_poolsize2_voi=df_merged_poolsize2_voi.replace({'Pf': 1})\n",
    "\n",
    "df_merged_poolsize2_voi.to_csv(\"ET_POOLED_EPI_VOI.csv\", index=False)\n",
    "\n",
    "#df_merged_poolsize_ind_pool.to_csv(\"ET_POOLED_EPI_QC.csv\", index=False)\n",
    "\n",
    "df_merged_poolsize_ind_pool_voi=df_merged_poolsize_ind_pool[df_merged_poolsize_ind_pool.G_ANNOTATION.isin(voi_list)]\n",
    "\n",
    "#df_merged_poolsize_ind_pool_voi.to_csv(\"ET_POOLED_EPI_QC_voi.csv\", index=False)\n",
    "\n",
    "df_merged_poolsize=df_merged_poolsize.replace({'xx': 1})\n",
    "\n",
    "df_merged_poolsize=df_merged_poolsize.replace({'Pf': 1})\n",
    "\n",
    "df_merged_poolsize[\"POOLEDSIZE\"] = pd.to_numeric(df_merged_poolsize[\"POOLEDSIZE\"])\n",
    "\n",
    "df_merged_poolsize[\"Prod\"]=df_merged_poolsize[\"VAF\"].str.split('%').str[0].str.strip(\"%\").astype(float)*df_merged_poolsize[\"POOLEDSIZE\"].astype(float) ##Get a product by multiplying variant frequency times the pool size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4fe20ed2-64e5-4d1e-a5f5-c0a4bda90f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/dlkj48z90j97v_lywzxm0kh00000gr/T/ipykernel_51306/2040610405.py:51: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_countv = df_merged_countv.drop('Prod', 1) ##drop POOLED information\n"
     ]
    }
   ],
   "source": [
    "#print(df_merged_poolsize)\n",
    "df_merged_count=df_merged_poolsize.groupby(['SITE','GENE','G_ANNOTATION', 'POOLED']).sum()  ##Sum the columns based on overlapping values on site, trackersnp, and POOLED\n",
    "#print(df_merged_count)\n",
    "df_merged_countv=df_merged_poolsize.groupby(['SITE','GENE','G_ANNOTATION', 'POOLED', 'SNP']).sum() ##Sum the columns based on agreeing values on site, trackersnp, and POOLED, and snp\n",
    "df_merged_count=df_merged_count.groupby(['SITE','GENE','G_ANNOTATION']).sum() ##Sum again based on stie and tracker snp\n",
    "df_merged_countv=df_merged_countv.groupby(['SITE','GENE','G_ANNOTATION','SNP']).sum() ##Sum again based on site, tracker, and snp\n",
    "#df_merged_countv = df_merged_countv.drop('Pools ', 1) ##drop POOLED information\n",
    "df_merged_countv=df_merged_countv.reset_index()  ##Reset index so  that we can  use the columns\n",
    "df_merged_countv['Type'] = np.where(df_merged_countv['SNP'].str[0].str.isdigit(), \"Mutation\" , \"WildType\") ##If the first character of SNP value is digit assign mutation  else assign wildtype\n",
    "#print(df_merged_countv)\n",
    "\n",
    "df_merged_countv.rename(columns={'POOLEDSIZE':'Number_of_samples'}, inplace=True) ##Reanme the Poolsize to number  of samples\n",
    "\n",
    "df_merged_countv['Tracker']=df_merged_countv['G_ANNOTATION'].str[:-1]\n",
    "df_merged_countvp=df_merged_countv  ##Create another dataframe from previous dataframe this is for calculating products for VAF\n",
    "df_merged_countv=df_merged_countv.pivot(index=[\"SITE\", 'GENE',\"G_ANNOTATION\"], columns=\"Type\", values=\"Number_of_samples\") ##pivot and align mutation and wildtype\n",
    "df_merged_countvp=df_merged_countvp.pivot(index=[\"SITE\", 'GENE',\"G_ANNOTATION\"], columns=\"Type\", values=\"Prod\") ##pivot and align mutation and wildtype\n",
    "\"\"\"\n",
    "To do: \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html\n",
    "This is where I am stuck. Trying using pivot to fix it\n",
    "\"\"\"\n",
    "\n",
    "df_merged_countvp=df_merged_countvp.reset_index() ##Reset indexes\n",
    "df_merged_countv=df_merged_countv.reset_index()  ##Reset indexes\n",
    "\n",
    "df_merged_countv['SNP'] = np.where(pd.isna(df_merged_countv['Mutation']), df_merged_countv['G_ANNOTATION'].astype(str).str[0:-1], df_merged_countv['G_ANNOTATION'].astype(str).str[1::]) ##Depending on codntion where mutation value is nan or not assign SNP with just first character or last character and amino acid position\n",
    "cols = list(df_merged_countv.columns) ##Change column to list format\n",
    "cols = cols[0:3] + cols[5:6] + cols[3:5] ##Adjust the column order by adding column as lists\n",
    "df_merged_countv = df_merged_countv[cols] ##Return the column back to dataframe\n",
    "df_merged_countv.rename(columns={'TrackerSNP':'G_ANNOTATION'}, inplace=True) ##Change column name from trackersnp to G_ANNOTATION\n",
    "\n",
    "#print(df_merged_countv)\n",
    "\n",
    "df_merged_countv['Prod']= df_merged_countvp['Mutation'] ##Copy the mutation information which is sum of all the variant frequencies for each variant\n",
    "\n",
    "df_merged_countv['Prod'] = df_merged_countv['Prod'].fillna(0) ##Fill empty values with 0s\n",
    "\n",
    "df_merged_countv['Mutation'] = df_merged_countv['Mutation'].fillna(0) ##Fill empty values with 0s\n",
    "\n",
    "df_merged_countv['WildType'] = df_merged_countv['WildType'].fillna(0) ##Fill empty values with 0s\n",
    "\n",
    "df_merged_countv[\"Total\"] = df_merged_countv['Mutation'] + df_merged_countv['WildType']                        # count total by adding mutation to Wildtype\n",
    "df_merged_countv[\"VAF\"] = (df_merged_countv['Prod']/df_merged_countv[\"Total\"]).round(2).astype(str) + '%'      # count vaf by dividing prod by total\n",
    "\n",
    "cols = list(df_merged_countv.columns)  ##turn columns in to list\n",
    "a, b = cols.index('VAF'), cols.index('Total') ##Switch reindex VAF and total columns\n",
    "cols[b], cols[a] = cols[a], cols[b]\n",
    "df_merged_countv = df_merged_countv[cols] ##Assign switched column orders\n",
    "\n",
    "df_merged_countv = df_merged_countv.drop('Prod', 1) ##drop POOLED information\n",
    "\n",
    "df_merged_countv=df_merged_countv.replace({'DHPS_437Corrected ': \"DHPS\"})\n",
    "\n",
    "df_voi=pd.read_csv(\"voinew3.csv\")\n",
    "df_voi[\"VOI\"]=df_voi[\"RefAA\"]+df_voi[\"AAPos\"].astype(str)+df_voi[\"AltAA\"]\n",
    "voi_list=df_voi[\"VOI\"].tolist()\n",
    "\n",
    "df_merged_countv=df_merged_countv[df_merged_countv.G_ANNOTATION.isin(voi_list)]\n",
    "\n",
    "df_merged_countv.to_csv(\"ET_weighted_bysite_EPI.csv\", sep=',', index=False)\n",
    "\n",
    "#print(df_merged_countviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3f670584-726b-4d2d-bf40-e50783c450a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pools 22ETAm 18\n",
      "sum of pools 22ETAm 159.0\n",
      "individual total samples 22ETAm 22\n"
     ]
    }
   ],
   "source": [
    "dicttest={} ## Create a test dictionary to check if items in the test dictionary or not\n",
    "dicttestind={} ##Create a dictionary for individuals to not count duplicatesd\n",
    "dictcat={} ##Dictionary for assining different sites\n",
    "\n",
    "with open(\"ET_pooled_EPI.csv\", \"r\") as r1: ##Loop thourgh pooled epi file\n",
    "    for lines in r1:\n",
    "        if lines[0:2].isdigit(): #and \"xx\" not in lines.split(\",\")[0]:\n",
    "            dictcat[lines[0:6]]=\"exist\" ##Create a dictionary for different sites based on first few strings of the lines\n",
    "with open(\"ET_individual_EPI.csv\", \"r\") as r2: ##Loop through individual epi file\n",
    "    for lines in r2:\n",
    "        if lines[0:2].isdigit(): #and \"xx\" not in lines.split(\",\")[0]:\n",
    "            dictcat[lines[0:6]]=\"exist\"  ##Create a dictionary for different sites based on first few strings of the lines\n",
    "\n",
    "for items in dictcat: ##Loop through different items, sites, with in the dictionary for site\n",
    "    dicttest={} ##Create an empty dictionary and fill up the dictionary test for just pooled samples\n",
    "    with open(\"ET_pooled_EPI.csv\", \"r\") as r1: ##Open the EPI file created from step 3-2\n",
    "        for lines in r1:    \n",
    "            if items in lines:\n",
    "                if \"POOLED\" in lines:\n",
    "                        dicttest[lines.split(\",\")[0]]=lines.split(\",\")[12] ##Assign pooled size to the dictionary\n",
    "                        \n",
    "        sumpooled=0\n",
    "        countpooled=0\n",
    "        for item in dicttest:\n",
    "            countpooled+=1\n",
    "            if dicttest[item]!=\"xx\\n\":\n",
    "                #print(dicttest[item])\n",
    "                sumpooled+=float(dicttest[item].strip(\"\\n\"))\n",
    "        print(\"Number of pools\", items, countpooled) ##Print out number of pools \n",
    "        print(\"sum of pools\", items, sumpooled) ##Print out sum of pools based on pool size for each pool\n",
    "for items in dictcat:\n",
    "    dicttestind={}  ##Create an empty dictionary and fill up the dictionary test for just individual samples\n",
    "    with open(\"ET_individual_EPI.csv\", \"r\") as r2:\n",
    "        for lines in r2:\n",
    "            if items in lines:\n",
    "                if \"individual\" in lines:\n",
    "                    dicttestind[lines.split(\",\")[0]]=\"exist\"\n",
    "        countindividual=0\n",
    "        for item in dicttestind:\n",
    "            countindividual+=1\n",
    "        print(\"individual total samples\", items, countindividual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01f053-5808-4383-ba9c-756214f89d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"ET_individual_EPI_VOI.csv\")\n",
    "\n",
    "\n",
    "# Check dataframe is correct \n",
    "df.head()\n",
    "\n",
    "# Print number of rows and columns; confirm its longform data\n",
    "print('The number of rows, columns:', df.shape) \n",
    "print('') # add space \n",
    "\n",
    "# Count number of unique samples in data \n",
    "uniq = df['ET_ID_IND']\n",
    "print('There are', uniq.nunique(), 'samples in this dataset.')\n",
    "print('') # add space \n",
    "\n",
    "# Check if there are any Null or NAs \n",
    "\n",
    "print('The number of null or NA values in data:')\n",
    "print(df.isnull().sum())\n",
    "print('') # add space \n",
    "\n",
    "# Sort data based on CODONS to ensure shown as ascending/descending in plot \n",
    "\n",
    "LS = df['G_ANNOTATION']  # Copy the full annotated SNPs into a new list called LS \n",
    "\n",
    "codon_num = [] # create an empty list \n",
    "\n",
    "# Loop through G_ANNOTATION list (LS) and strip first and last character \n",
    "for x in LS: \n",
    "    codon_num.append(int(x[1:-1]))  # IMPORTANTLY change from string to integer to allow num sorting \n",
    "    ## TODO: make this more pythonic and use rstrip() and lstrip().left()\n",
    "\n",
    "# Add the new Codon column to the current dataframe \n",
    "df['Codon'] = codon_num\n",
    "\n",
    "# Set seaborn plot style and size (NOTE: some plots will not be affected; see seaborn docs) \n",
    "\n",
    "#sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set plot/figure size \n",
    "sns.set(rc = {'figure.figsize':(15,15)})\n",
    "\n",
    "print('Plotting all SNPs categorized as wildtype or mutant using seaborn:')\n",
    "\n",
    "# Plot as seaborn strippplot sorting by Codon # and then Gene in ascending order \n",
    "\n",
    "g= sns.stripplot(data=df.sort_values(by=['GENE','Codon'], ascending=True),\n",
    "               x=\"TYPE\", y=\"G_ANNOTATION\",\n",
    "                 hue=\"GENE\")  \n",
    "\n",
    "\n",
    "\n",
    "# Use catplot() to combine stripplot() and FacetGrid to further categorize data; below example is by Year \n",
    "\n",
    "# g= sns.catplot(data=df.sort_values(by=['GENE','Codon'], ascending=True),\n",
    "#                x=\"TYPE\", y=\"G_ANNOTATION\", \n",
    "#                hue=\"GENE\", col=\"Year\")  \n",
    "\n",
    "##TODO: \n",
    "## Adjsust size for catplot, difficult to see as is. Need to look at seaborn docs. \n",
    "## Figure out why sorting is broken (might to do something with iteration of catplot() \n",
    "\n",
    "#Save dataframe to csv \n",
    "\n",
    "# df.to_csv('organized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "25f1dcca-72a6-4895-a751-dc8060775a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 0 samples that are over total individauls by site\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dictcat={}\n",
    "with open(\"ET_individual_EPI.csv\", \"r\") as r2: ##Open individual EPI file for QC of EPI individaul\n",
    "    for lines in r2:\n",
    "        if lines[0:2].isdigit(): #and \"xx\" not in lines.split(\",\")[0]: ##Check if first two letters are digit to not count the controls\n",
    "            dictcat[lines[0:6]]=\"exist\" ##Assign first 6 letters of line as one of the categories\n",
    "\n",
    "dict_site={} ##Create dictionary for site\n",
    "for items in dictcat:\n",
    "    dicttestind={}  ##Create dictionary for individual samples\n",
    "    with open(\"ET_individual_EPI.csv\", \"r\") as r2:\n",
    "        for lines in r2:\n",
    "            if items in lines:\n",
    "                dicttestind[lines.split(\",\")[0]]=\"exist\" ##Assign different samples to the dictionary to prevent duplicates\n",
    "        countindividual=0\n",
    "        for item in dicttestind:\n",
    "            countindividual+=1 ##Count the number of individuals and assign total individual counted to for each site\n",
    "        dict_site[items]=countindividual\n",
    "    \n",
    "with open (\"ET_individual_EPI.csv\", \"r\") as t1:\n",
    "    dict_Samples={} ##dictionary for all the samples\n",
    "    dict_Genes={}\n",
    "    count=0\n",
    "    for lines in t1: ##Add number of major, minor, \n",
    "        count+=1\n",
    "        if count>1:\n",
    "            dict_Genes[lines.split(\",\")[0],lines.split(\",\")[2],lines.split(\",\")[9],lines.split(\",\")[11].strip(\"\\n\")]=\"exist\"  ##Assign sample name, site, G_ANNOTATION, and type of mutation to dictionary of genes\n",
    "\n",
    "\n",
    "dict_count={} ##Crate a dictionary dict_count so that we could add count of genes based on it existing in the dictionary or not. So if it doesn't exist just assisgn 1 other wise add 1 to the current numbers\n",
    "for items2 in dict_Genes:\n",
    "    if (items2[1],items2[2]) in dict_count:    \n",
    "        dict_count[items2[1],items2[2]]=dict_count[items2[1],items2[2]]+1\n",
    "    if (items2[1],items2[2]) not in dict_count:\n",
    "        dict_count[items2[1],items2[2]]=1\n",
    "\n",
    "dict_site_trans={} ##Based on dict_site created before create a translator dicttionary for different sites to different sum of item\n",
    "for items in dict_site:\n",
    "    if \"Am\" in items:\n",
    "        dict_site_trans[\"Amhara\"]=dict_site[items]\n",
    "\n",
    "count=0\n",
    "for items in dict_count:\n",
    "    if dict_count[items]>dict_site_trans[items[0]]: ##Check if there were samples for each site that is larger than total number of samples.\n",
    "        count+=1\n",
    "        \n",
    "print(\"There were\", count,\"samples that are over total individauls by site\")\n",
    "            \n",
    "df=pd.DataFrame.from_dict(dict_count, orient='index')\n",
    "df.to_csv(\"categorical_test1.csv\", sep='\\t')\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d03265-e098-4de8-9f46-f572598967ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ET_weighted_bysite_EPI.csv\")\n",
    "\n",
    "df['Type'] = [\"Wildtype: AF=0%\" if float(x.strip(\"%\"))==0 else 'Minor: AF < 50%' if 0<float(x.strip(\"%\"))<0.5 else 'Major: AF >= 50%' for x in df['VAF']]\n",
    "#print(df)\n",
    "df_viz = pd.DataFrame()\n",
    "df_type = pd.DataFrame()\n",
    "#df.groupby([\"GENE\",\"G_annotation\",\"Type\"]).sum()\n",
    "#print(df.groupby([\"GENE\",\"G_annotation\"]).sum())\n",
    "\n",
    "#df_viz[\"GENE\"]=df.groupby([\"GENE\",\"G_annotation\"]).sum().reset_index()[\"GENE\"]\n",
    "#df_viz[\"G_annotation\"]=df.groupby([\"GENE\",\"G_annotation\"]).sum().reset_index()[\"G_annotation\"]\n",
    "#df_viz[\"Total\"]=df.groupby([\"GENE\",\"G_annotation\"]).sum().reset_index()[\"Total\"]\n",
    "df_type[\"GENE\"]=df.groupby([\"GENE\",\"G_ANNOTATION\",\"Type\"]).sum().reset_index()[\"GENE\"]\n",
    "df_type[\"G_ANNOTATION\"]=df.groupby([\"GENE\",\"G_ANNOTATION\",\"Type\"]).sum().reset_index()[\"G_ANNOTATION\"]\n",
    "df_type[\"Type\"]=df.groupby([\"GENE\",\"G_ANNOTATION\",\"Type\"]).sum().reset_index()[\"Type\"]\n",
    "#df_type[\"Total\"]=df.groupby([\"GENE\",\"G_ANNOTATION\",\"Type\"]).sum().reset_index()[\"Total\"]\n",
    "df_type[\"Mutation\"]=df.groupby([\"GENE\",\"G_ANNOTATION\",\"Type\"]).sum().reset_index()[\"Mutation\"]\n",
    "df_type[\"WildType\"]=df.groupby([\"GENE\",\"G_ANNOTATION\",\"Type\"]).sum().reset_index()[\"WildType\"]\n",
    "\n",
    "#print(df_viz)\n",
    "#print(df_viz)\n",
    "#print(df_type.columns)\n",
    "#print(df_viz)\n",
    "\n",
    "df_type[\"Major: AF >= 50%\"]=np.where(df_type[\"Type\"]==\"Major: AF >= 50%\", df_type[\"Mutation\"], 0)\n",
    "df_type[\"Minor: AF < 50%\"]=np.where(df_type[\"Type\"]==\"Minor: AF < 50%\", df_type[\"Mutation\"], 0)\n",
    "df_type[\"Wildtype: AF=0%\"]=df_type[\"WildType\"]\n",
    "\n",
    "print(df_type)\n",
    "df_type=df_type.drop(['Type'], axis=1)\n",
    "\n",
    "print(df_type)\n",
    "df_final=df_type.groupby([\"GENE\",\"G_ANNOTATION\"]).sum().reset_index()\n",
    "\n",
    "df_final[\"GENE_G_anno\"]=df_final[\"GENE\"]+\":\"+df_final[\"G_ANNOTATION\"]\n",
    "\n",
    "df_final=df_final.drop(['GENE','G_ANNOTATION'], axis=1)\n",
    "df_final=df_final.drop(['Mutation','WildType'], axis=1)\n",
    "\n",
    "df_final[\"Total\"]=df_final[\"Wildtype: AF=0%\"]+df_final[\"Major: AF >= 50%\"]+df_final[\"Minor: AF < 50%\"]  # \n",
    "\n",
    "\n",
    "cols = list(df_final.columns)\n",
    "a, b = cols.index('GENE_G_anno'), cols.index('Major: AF >= 50%')\n",
    "cols[b], cols[a] = cols[a], cols[b]\n",
    "df_final = df_final[cols]\n",
    "\n",
    "\n",
    "df_final[\"Major: AF >= 50%\"]=df_final[\"Major: AF >= 50%\"]/df_final[\"Total\"]\n",
    "df_final[\"Minor: AF < 50%\"]=df_final[\"Minor: AF < 50%\"]/df_final[\"Total\"]\n",
    "df_final[\"Wildtype: AF=0%\"]=df_final[\"Wildtype: AF=0%\"]/df_final[\"Total\"]\n",
    "\n",
    "df_final.to_csv(\"Tab_Table_SNP_Total1_DF.csv\", index=False)\n",
    "\n",
    "#print(df_final)\n",
    "\n",
    "#print(df_type)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e305b-701e-4cc3-ac93-2270f36f3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "##Used the previously created table\n",
    "df_table_SNP=pd.read_csv(\"Tab_Table_SNP_Total1_DF.csv\")\n",
    "#print(df_table_SNP)\n",
    "##Create different dataframe by adding different columns of raios\n",
    "#df_table_SNP.rename(columns={'Gene+SNP':'GeneSNP'}, inplace=True) ##Change column name from trackersnp to G_ANNOTATION\n",
    "#df_table_SNP.groupby(df_table_SNP.GeneSNP.str.split(\":\")[0])\n",
    "df_table_SNP=df_table_SNP.sort_values(by=['GENE_G_anno'])\n",
    "#print(df_table_SNP.GENE_G_anno.str.split(\":\").str[1])\n",
    "#print(df_table_SNP.GENE_G_anno.str.split(\":\").str[1].str[1:-1])\n",
    "df_table_SNP[\"index\"]=df_table_SNP.GENE_G_anno.str.split(\":\").str[1].str[1:-1]\n",
    "df_table_SNP[\"index\"]=df_table_SNP[\"index\"].astype(int)\n",
    "df_table_SNP[\"index2\"]=df_table_SNP.GENE_G_anno.str.split(\":\").str[0]\n",
    "\n",
    "df_table_SNP=df_table_SNP.sort_values([\"index\"])\n",
    "#df.groupby('A', sort=False)\n",
    "df_table_SNP=df_table_SNP.groupby(['index2'], sort=False)\n",
    "df_table_SNP=pd.concat(map(lambda x: x[1], df_table_SNP))\n",
    "#print(df_table_SNP)\n",
    "#df_table_SNP=df_table_SNP.sort_values([\"index2\"])\n",
    "#print(df)\n",
    "#df_table_SNP=df_table_SNP.sort_values(by=[\"index\"])\n",
    "#df_table_SNP=df_table_SNP.sort_values([\"index2\"])\n",
    "#print(df_table_SNP.sort_values(by=[\"index\",\"index2\"]))\n",
    "\n",
    "#print(idx)\n",
    "#df_table_SNP = df_table_SNP.reindex(idx).reset_index(drop=True)\n",
    "#print(df_table_SNP)\n",
    "#idx = df['email'].str.split('@', expand=True).sort_values([1,0]).index\n",
    "#df_table_SNP=df_table_SNP.GENE_G_anno.str.split(\":\").str[1].str[1:-1].sort_values().index\n",
    "#df_table_SNP = df_table_SNP.reindex(df_table_SNP).reset_index(drop=True)\n",
    "#print(sorted(df_table_SNP, key=lambda x: df_table_SNP.GENE_G_anno.str.split(\":\").str[1].str[1:-1]))\n",
    "df_table_SNP[\"Total\"]=df_table_SNP[\"Total\"].astype(str)\n",
    "df_table_SNP[\"NewCol\"]=df_table_SNP[\"GENE_G_anno\"]+\":\"+\"N=\"+df_table_SNP[\"Total\"] ##Create the label for y-axis \n",
    "###The visualization works by adding different number of bars \n",
    "###There are three bars (total1 which has all major, minor, wildtypes, total2, just wildtype and major, and just wildtype\n",
    "###The purpose is to create a stacked bar plots\n",
    "df_table_SNP[\"total\"]=df_table_SNP[\"Minor: AF < 50%\"]+df_table_SNP[\"Major: AF >= 50%\"]+df_table_SNP[\"Wildtype: AF=0%\"] ##Create total for adding up all the bars by adding all the values for major minor and wildtype \n",
    "df_table_SNP[\"total2\"]=df_table_SNP[\"Major: AF >= 50%\"]+df_table_SNP[\"Wildtype: AF=0%\"]\n",
    "f, ax = plt.subplots(figsize = (9,11))\n",
    "sns.set_color_codes('pastel')\n",
    "##Once dataframes are created just plot the dataframe to the graph\n",
    "ax = sns.barplot(x=\"total\", y=\"NewCol\", label = \"Minor: AF < 50%\", data=df_table_SNP, color = 'r', edgecolor = 'w')\n",
    "ax = sns.barplot(x=\"total2\", y=\"NewCol\", label= \"Major: AF >= 50%\", data=df_table_SNP, color = 'g', edgecolor = 'w')\n",
    "sns.set_color_codes('deep')\n",
    "ax = sns.barplot(x=\"Wildtype: AF=0%\", y=\"NewCol\", label= \"Wildtype: AF=0%\", data=df_table_SNP, color = 'b', edgecolor = 'w')\n",
    "ax.legend(ncol = 2, loc = 'lower right')\n",
    "sns.despine(left = True, bottom = True)\n",
    "ax.set(ylabel=\"Gene+SNP+#_of_Samples\")\n",
    "ax.set(xlabel=\"Types of mutations ratio\")\n",
    "plt.legend(loc=(1.04,0))\n",
    "ax.set(title=\"Drug Resistance SNPs for Ethiopia  \", ylabel=\"SNPs\", xlabel=\"SNP ratio\")\n",
    "\n",
    "plt.savefig('ET_Bar_plot_Combined3', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4abe1e-9eb7-460b-9793-539e3126a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8976a8d2-80d6-4ea6-bf27-71fd93a3b763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6dc57b-40ed-4a76-a691-b2bf8b479c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
