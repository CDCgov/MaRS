{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e292b979-e7de-47e5-b6e6-f3795c44400b",
   "metadata": {},
   "source": [
    "## Geneious analysis for individual samples from raw Geneious output, \"Annotation.csv\" \n",
    "\n",
    " ### Required packages\n",
    " - No specific package required \n",
    " \n",
    " ### Inputs\n",
    " - Geneious SNP analysis of _k13_, _crt_, _mdr1_, _dhfr_, _dhps_, and _cytb_\n",
    " - Documentation on Geneious analysis can be found: Readme.md\n",
    " - Geneious outputs were modified to GuineaAnalysis_Individual.csv from \"Annotation.csv\"\n",
    " \n",
    " \n",
    " ### Data structure \n",
    " - [Long-form](https://seaborn.pydata.org/tutorial/data_structure.html#long-form-vs-wide-form-data) \n",
    "     - Each variable is a column \n",
    "\n",
    "         - \"Sample\" = *AMD ID*, including associated meta-data for each sample\n",
    "             - AMD ID and bit code key is found under MS Teams > Domestic > Files > Sample Naming > Sample_naming_key.pptx  \n",
    "\n",
    "             - Key: **Year Country State/Site DayofTreatment Treatment SampleID Genus SampleType GeneMarker-8bitcode SampleSeqCount**\n",
    "\n",
    "                 - Example:\n",
    "                     - Individual sequenced sample ID: 17GNDo00F0001PfF1290 = 2017 Guinea Dorota Day0 AS+AQ 0001 P.falciparum FilterBloodSpot k13-crt-mdr-dhfr-dhps-cytB-cpmp-pfs47 \n",
    "\n",
    "                     - Pooled sequenced sample ID: 17GNDoxxx001P10F1290 = 2017 Guinea Dorota **xx x** 001 **Pooled SamplesInPool** P.falciparum FilterBloodSpot k13-crt-mdr-dhfr-dhps-cytB-cpmp-pfs47 \n",
    "\n",
    "                         - NOTE: If information is not availble (na) **x** is used. For pooled samples, DayofTreatment and Treatment is na since its a pool of multiple samples with that info. \n",
    "                         - NOTE: For pooled samples, **Genus** is replaced with **Pooled** and **SampleType** with **SamplesInPool** to indicated this as a pooled sequenced sample and sample count in each pool. \n",
    "         <p>&nbsp;</p>\n",
    "         - \"Year\" = the year the study was conducted \n",
    "         - \"Site\" = the state or province \n",
    "         - \"Day_of_treatment\" = describes the day of treatment provided to the patient \n",
    "         - \"Gene\" = drug resistant gene(s) \n",
    "         - \"G_annotation\" = full SNP annotation in the following format: WildTypeAA-CodonPosition-MutantAA \n",
    "         - \"Coverage\" = the number of reads covering the SNP \n",
    "         - \"VAF\" = variant allele frequency calculated by AA divided by total reads in loci \n",
    "         - \"SNP\" = single nucleotide polymorphism in WildTypeAA or MutantAA annotation format \n",
    "         - \"Type\" = describes if it is a wild type or mutant SNP \n",
    "\n",
    "     - Each observation is a row for each sample ID (patient ID) \n",
    " \n",
    " #### TODO\n",
    " \n",
    " #### Activity Name\n",
    " - [ ] Write doc.string at the beginning of the code\n",
    " - [ ] Write detailed description with comment for line by line\n",
    " - [ ] Make the code more simple and accurate\n",
    " - [ ] Follow zen of python\n",
    "    \n",
    " #### Completed Activity âœ“\n",
    " - [x] Created marked down at the beginning of the file for description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "83bfdd5a-14d1-4bdd-bfac-48e4f7ac2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146/2718144918.py:3: DtypeWarning: Columns (16,19,24,26,28,30,31,32,33,34,35,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,57,58,59,60,61,62,64,65,66,67,68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Geneious_DF=pd.read_csv(\"Annotations.csv\") ##Import raw Geneious output for variant analysis\n",
      "/tmp/ipykernel_146/2718144918.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Geneious_DF_N1[\"TrackerSNP\"]=Geneious_DF_N1[\"Amino Acid Change\"].astype(str).str[0]+Geneious_DF_N1[\"CDS Codon Number\"].astype(int).astype(str)+Geneious_DF_N1[\"Amino Acid Change\"].astype(str).str[-1] ##Create a TrackerSNP Column which has both amino acid before the change and after the change\n",
      "/tmp/ipykernel_146/2718144918.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"SITE\"]=Combination_filtered.apply(site, axis=1) ##Apply the functions defined previously\n",
      "/tmp/ipykernel_146/2718144918.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"TreatmentDay\"]=Combination_filtered.apply(TreatmentDay, axis=1)\n",
      "/tmp/ipykernel_146/2718144918.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"Pooled\"]=Combination_filtered.apply(Pooled, axis=1)\n",
      "/tmp/ipykernel_146/2718144918.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"Year\"]=Combination_filtered.apply(year, axis=1)\n",
      "/tmp/ipykernel_146/2718144918.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"TYPE\"]=Combination_filtered.apply(type, axis=1)\n",
      "/tmp/ipykernel_146/2718144918.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"SNP\"]=Combination_filtered.apply(SNP, axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd ## Import Pandas library for processing dataframe as pd\n",
    "import numpy as np ## Import Numy for processing matrix as np\n",
    "Geneious_DF=pd.read_csv(\"Annotations.csv\") ##Import raw Geneious output for variant analysis\n",
    "Geneious_DF_N1=Geneious_DF[(Geneious_DF['Type']=='Polymorphism') & (Geneious_DF['Amino Acid Change'].notnull())] ##If the type column contains polymorphism and Amino Acid Change column is not empty then create a dataframe satisfying those conditions\n",
    "\n",
    "Geneious_DF_N2=Geneious_DF[Geneious_DF['Type']=='Coverage - High'] ##If the Coverage - High is in the type column as value then select dataframe for those column values\n",
    "\n",
    "Geneious_DF_N1[\"TrackerSNP\"]=Geneious_DF_N1[\"Amino Acid Change\"].astype(str).str[0]+Geneious_DF_N1[\"CDS Codon Number\"].astype(int).astype(str)+Geneious_DF_N1[\"Amino Acid Change\"].astype(str).str[-1] ##Create a TrackerSNP Column which has both amino acid before the change and after the change\n",
    "\n",
    "Combine_Variant_Wildtpye = [Geneious_DF_N1, Geneious_DF_N2] ##Produce a complete dataframe which contains both variants and wildtypes\n",
    "Combation_Vi_Wi = pd.concat(Combine_Variant_Wildtpye) ##Concatenate the dataframes for variants and wildtypes\n",
    "Combination_filtered=Combation_Vi_Wi.drop_duplicates(subset =[\"Document Name\", \"TrackerSNP\"] )  ##Drop duplicates meaning if the values are already in variants then drop it from the wildtypes\n",
    "##The dataframe contains information, \"Sample,Pooled,Year,SITE,TreatmentDay,GENE,G_annotation,COVERAGE,VAF,VF,SNP,TYPE\\n\")\n",
    "\n",
    "def site(row): ##Set up a function for assignging site based on the values in the document name column. This will change with each study please update the sites. \n",
    "    if row['Document Name'][4:6]==\"Be\":\n",
    "        return 'Benguela'\n",
    "    elif row['Document Name'][4:6]==\"LS\":\n",
    "        return 'Lunda Sul'\n",
    "    elif row['Document Name'][4:6]==\"Za\":\n",
    "        return 'Zaire'\n",
    "    \n",
    "def TreatmentDay(row): ##Set up a function for assigning TreatmentDay based on the values in the document name column\n",
    "    if row['Document Name'][6:8]==\"00\":\n",
    "        return '0'\n",
    "    elif row['Document Name'][6:8]==\"1A\": ##To do: confirm this with Stefano whether it was a naming issue.\n",
    "        return '1'\n",
    "    elif row['Document Name'][6:8]!=\"00\" and row['Document Name'][6:8]!=\"1A\":\n",
    "        return row['Document Name'][6:8]\n",
    "    \n",
    "def Pooled(row): ##Set up a function for Pooled based on the values in the document name column\n",
    "    if row['Document Name'][8:10]==\"xp\":\n",
    "        return 'pooled'\n",
    "    elif row['Document Name'][8:10]!=\"xp\":\n",
    "        return 'individual'\n",
    "\n",
    "def year(row):  ##Set up a function for Year  based on the values in the document name column\n",
    "    return row['Document Name'][0:2]\n",
    "\n",
    "def type(row):  ##Set up a TYPE column based on given value in the Type whether it is mutation or wildtype\n",
    "    if row['Type'] =='Polymorphism':\n",
    "        return \"mutation\"\n",
    "    if row['Type'] =='Coverage - High':\n",
    "        return \"wildtype\"\n",
    "    \n",
    "def SNP(row):  ##Set up a SNP column to give pre or post amino acid changes based on mutation or wildtype\n",
    "    if row['Type'] =='Polymorphism':\n",
    "        return row['TrackerSNP'][1::]\n",
    "    if row['Type'] =='Coverage - High':\n",
    "        return row['TrackerSNP'][0:-1]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "Combination_filtered[\"SITE\"]=Combination_filtered.apply(site, axis=1) ##Apply the functions defined previously\n",
    "Combination_filtered[\"TreatmentDay\"]=Combination_filtered.apply(TreatmentDay, axis=1)\n",
    "Combination_filtered[\"Pooled\"]=Combination_filtered.apply(Pooled, axis=1)\n",
    "Combination_filtered[\"Year\"]=Combination_filtered.apply(year, axis=1)\n",
    "Combination_filtered[\"TYPE\"]=Combination_filtered.apply(type, axis=1)\n",
    "Combination_filtered[\"SNP\"]=Combination_filtered.apply(SNP, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "Combination_report1=Combination_filtered[Combination_filtered['Type']=='Polymorphism'] ##Select columns with mutations\n",
    "Combination_report2=Combination_filtered[Combination_filtered['Type']=='Coverage - High'] ##Select columns with wildtypes\n",
    "final_report1=Combination_report1[[\"Document Name\",\"Pooled\",\"Year\",\"SITE\",\"TreatmentDay\",\"Sequence Name\",\"TrackerSNP\",\"Coverage\",\"Variant Frequency\",\"Variant Raw Frequency\",\"TYPE\",\"SNP\"]] ##Assign sample information to samples with mutation\n",
    "final_report2=Combination_report2[[\"Document Name\",\"Pooled\",\"Year\",\"SITE\",\"TreatmentDay\",\"Sequence Name\",\"TrackerSNP\",\"Average Coverage\",\"Variant Frequency\",\"Variant Raw Frequency\",\"SNP\",\"TYPE\"]] ##Assign sample information to samples with wildtypes\n",
    "final_report2_re=final_report2.rename(columns={'Average Coverage': 'Coverage'}) ##Change the name of average coverage to coverage for samples with wildtypes\n",
    "final_combine=[final_report1, final_report2_re] ##Combine the information from wildtypes and mutations into one dataframe\n",
    "final_combine_2=pd.concat(final_combine) ##concatenate\n",
    "final_combine_2.to_csv(\"test.csv\", sep=',', index=False) ##Create a file with the dataframe for testing purpose\n",
    "\n",
    "df_voi=pd.read_csv(\"voinew3.csv\")\n",
    "df_voi[\"VOI\"]=df_voi[\"RefAA\"]+df_voi[\"AAPos\"].astype(str)+df_voi[\"AltAA\"]\n",
    "voi_list=df_voi[\"VOI\"].tolist()\n",
    "\n",
    "final_combine_3=final_combine_2[final_combine_2[\"Pooled\"]==\"individual\"]\n",
    "final_combine_3=final_combine_3[final_combine_3.TrackerSNP.isin(voi_list)]\n",
    "final_combine_3=final_combine_3.rename(columns={'Document Name': 'Sample'})\n",
    "final_combine_3=final_combine_3.rename(columns={'Sequence Name': 'GENE'})\n",
    "final_combine_3=final_combine_3.rename(columns={'TrackerSNP': 'G_annotation'})\n",
    "final_combine_3=final_combine_3.rename(columns={'Variant Frequency': 'VAF'})\n",
    "final_combine_3=final_combine_3.rename(columns={'Variant Raw Frequency': 'VF'})\n",
    "final_combine_3=final_combine_3.rename(columns={'Coverage': 'COVERAGE'})\n",
    "final_combine_3=final_combine_3.rename(columns={'Coverage': 'COVERAGE'})\n",
    "\n",
    "cols = list(final_combine_3.columns)\n",
    "a, b = cols.index('SNP'), cols.index('TYPE')\n",
    "cols[b], cols[a] = cols[a], cols[b]\n",
    "final_combine_3 = final_combine_3[cols]\n",
    "\n",
    "\n",
    "\n",
    "final_combine_3.to_csv(\"AN19_individual_EPI.csv\", sep=',', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bc46cb9f-c06f-435b-96cb-45ebd104f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146/4000270356.py:16: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_poolsize = df_merged_poolsize.drop('SITE ', 1) ##Get rid of duplicate columns which is SITE_y\n",
      "/tmp/ipykernel_146/4000270356.py:17: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_poolsize = df_merged_poolsize.drop('Year_y', 1) ##Get rid of duplicate columns which is year\n",
      "/tmp/ipykernel_146/4000270356.py:41: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_poolsize_2 = df_merged_poolsize_2.drop('Prod', 1)\n"
     ]
    }
   ],
   "source": [
    "pooled_part1=pd.read_csv(\"Pooled_Info.csv\") ##import a file with information about pooled samples\n",
    "#Combation_pooled_concatenate.to_csv(\"test-pre1.csv\", sep=',', index=False) ##create file for testing purpose\n",
    "\n",
    "def name(row):\n",
    "    return row['Document Name'].split(\"_\")[0]\n",
    "\n",
    "final_combine_2[\"Document Name\"]=final_combine_2.apply(name, axis=1) ##Clean the document name which is the AMD_ID to get rid of the Geneious information\n",
    "\n",
    "final_combine_2.rename(columns={'Document Name':'AMD ID (Pooled)'}, inplace=True) ##Reanme the document name to AMD_ID\n",
    "\n",
    "\n",
    "df_merged_poolsize = pd.merge(final_combine_2, pooled_part1, on=['AMD ID (Pooled)'], how='left') ##merge the information based on AMD_ID to add pooled columns and pooled size columns\n",
    "\n",
    "#df_merged_poolsize.to_csv(\"test2.csv\", sep=',', index=False) ##Generate file to test\n",
    "\n",
    "df_merged_poolsize = df_merged_poolsize.drop('SITE ', 1) ##Get rid of duplicate columns which is SITE_y\n",
    "df_merged_poolsize = df_merged_poolsize.drop('Year_y', 1) ##Get rid of duplicate columns which is year\n",
    "\n",
    "df_merged_poolsize.PoolSize.fillna(value=1, inplace=True) ##Fill empty values for poolsize for individual with 1s\n",
    "\n",
    "df_merged_poolsize['Variant Frequency'] = df_merged_poolsize['Variant Frequency'].fillna(0)\n",
    "\n",
    "#print(df_merged_poolsize[\"Variant Frequency\"].str.split('%').str[0].str.strip(\"%\"))\n",
    "\n",
    "df_merged_poolsize[\"Prod\"]=df_merged_poolsize[\"Variant Frequency\"].str.split('%').str[0].str.strip(\"%\").astype(float)*df_merged_poolsize[\"PoolSize\"].astype(float)\n",
    "\n",
    "df_merged_poolsize.to_csv(\"test2.csv\", sep=',', index=False) ##Generate file to test\n",
    "\n",
    "df_voi=pd.read_csv(\"voinew3.csv\")\n",
    "df_voi[\"VOI\"]=df_voi[\"RefAA\"]+df_voi[\"AAPos\"].astype(str)+df_voi[\"AltAA\"]\n",
    "voi_list=df_voi[\"VOI\"].tolist()\n",
    "\n",
    "df_merged_poolsize=df_merged_poolsize[df_merged_poolsize.TrackerSNP.isin(voi_list)]\n",
    "\n",
    "#df_merged_poolsize = df_merged_poolsize.drop('Prod', 1) ##drop pooled information\n",
    "\n",
    "df_merged_poolsize.to_csv(\"test2_EP.csv\", sep=',', index=False) ##Generate file to test\n",
    "\n",
    "\n",
    "df_merged_poolsize_2=df_merged_poolsize[df_merged_poolsize[\"Pooled\"]==\"pooled\"]\n",
    "df_merged_poolsize_2 = df_merged_poolsize_2.drop('Prod', 1)\n",
    "\n",
    "df_merged_poolsize_2=df_merged_poolsize_2.rename(columns={'AMD ID (Pooled)': 'Sample'})\n",
    "df_merged_poolsize_2=df_merged_poolsize_2.rename(columns={'Sequence Name': 'GENE'})\n",
    "df_merged_poolsize_2=df_merged_poolsize_2.rename(columns={'TrackerSNP': 'G_annotation'})\n",
    "df_merged_poolsize_2=df_merged_poolsize_2.rename(columns={'Variant Frequency': 'VAF'})\n",
    "df_merged_poolsize_2=df_merged_poolsize_2.rename(columns={'Variant Raw Frequency': 'VF'})\n",
    "df_merged_poolsize_2=df_merged_poolsize_2.rename(columns={'Coverage': 'COVERAGE'})\n",
    "df_merged_poolsize_2=df_merged_poolsize_2.rename(columns={'Coverage': 'COVERAGE'})\n",
    "\n",
    "cols = list(df_merged_poolsize_2.columns)\n",
    "a, b = cols.index('SNP'), cols.index('TYPE')\n",
    "cols[b], cols[a] = cols[a], cols[b]\n",
    "df_merged_poolsize_2 = df_merged_poolsize_2[cols]\n",
    "\n",
    "cols = list(df_merged_poolsize_2.columns)\n",
    "a, b = cols.index('Pools '), cols.index('PoolSize')\n",
    "cols[b], cols[a] = cols[a], cols[b]\n",
    "df_merged_poolsize_2 = df_merged_poolsize_2[cols]\n",
    "\n",
    "df_merged_poolsize_2=df_merged_poolsize_2.rename(columns={'Pools ': 'PooledGroup'})\n",
    "df_merged_poolsize_2=df_merged_poolsize_2.rename(columns={'PoolSize': 'PooledSize'})\n",
    "df_merged_poolsize_2=df_merged_poolsize_2.rename(columns={'Year_x': 'Year'})\n",
    "df_merged_poolsize_2=df_merged_poolsize_2.rename(columns={'TYPE': 'Type'})\n",
    "\n",
    "import math\n",
    "\n",
    "with open(\"AN19_pooled_EPI_fixed.csv\", \"w\") as w1:\n",
    "    count=0\n",
    "    #print(len(df_merged_poolsize_2))\n",
    "    for items in list(df_merged_poolsize_2):\n",
    "        count+=1\n",
    "        if count==1:\n",
    "            w1.write(str(items))\n",
    "        if count>1:\n",
    "            w1.write(\",\"+str(items))\n",
    "        if count==14:\n",
    "            w1.write(\",\"+str(items)+\"\\n\")\n",
    "            break\n",
    "        #print(items)\n",
    "    for index, row in df_merged_poolsize_2.iterrows():\n",
    "        count=0\n",
    "        #print(len(row))\n",
    "        for item in row:\n",
    "            #print(item)\n",
    "            count+=1\n",
    "            if count==1:\n",
    "                w1.write(str(item))\n",
    "            if count>1:\n",
    "                if pd.isnull(item):\n",
    "                    #print(item)\n",
    "                    w1.write(\",\"+\"0\")\n",
    "                if \"->\" in str(item):\n",
    "                    w1.write(\",\"+item.split(\"-\")[0][0:-1])\n",
    "                if \"->\" not in str(item) and pd.isnull(item)==False:\n",
    "                    w1.write(\",\"+str(item))\n",
    "            if count==len(row):\n",
    "                w1.write(\",\"+str(item)+\"\\n\")\n",
    "                break\n",
    "            #ount+=1\n",
    "\n",
    "\n",
    "#df_merged_poolsize_2.to_csv(\"AN19_pooled_EPI.csv\", sep=',', index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbdafdd2-4e85-441f-a444-7ae3318ad7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146/1448223827.py:10: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_countv = df_merged_countv.drop('Pools ', 1) ##drop pooled information\n",
      "/tmp/ipykernel_146/1448223827.py:56: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_countv = df_merged_countv.drop('Prod', 1) ##drop pooled information\n",
      "/tmp/ipykernel_146/1448223827.py:57: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_countv = df_merged_countv.drop('sum', 1) ##drop pooled information\n"
     ]
    }
   ],
   "source": [
    "#df_merged_poolsize = df_merged_poolsize[~df_merged_poolsize.AMD_ID.isin([\"17GNHa16F0011PfF1290\",\"17GNHa21F0165PfF1290\",\"18GNMa21A0014PfF1290\",\"18GNMa1A0129PfF1290\",\"18GNLS21A0012PfF1290\",\"18GNLS21A0103PfF1290\",\"18GNLS21A0136PfF1290\",\"18GNLS14F0141PfF1290\",\"19GNHa14A0077PfF1290\"])] ##check for day of failure for recrudescence\n",
    "#[\"17GNHa16F0011PfF1290\",\"17GNHa21F0165PfF1290\",\"18GNMa21A0014PfF1290\",\"18GNMa1A0129PfF1290\",\"18GNLS21A0012PfF1290\",\"18GNLS21A0103PfF1290\",\"18GNLS21A0136PfF1290\",\"18GNLS14F0141PfF1290\",\"19GNHa14A0077PfF1290\"]\n",
    "\n",
    "#df_merged_poolsize=df_merged_poolsize.reset_index()\n",
    "\n",
    "df_merged_count=df_merged_poolsize.groupby(['SITE','Sequence Name','TrackerSNP', 'Pooled']).sum()  ##Sum the columns based on overlapping values on site, trackersnp, and pooled\n",
    "df_merged_countv=df_merged_poolsize.groupby(['SITE','Sequence Name','TrackerSNP', 'Pooled', 'SNP']).sum() ##Sum the columns based on agreeing values on site, trackersnp, and pooled, and snp\n",
    "df_merged_count=df_merged_count.groupby(['SITE','Sequence Name','TrackerSNP']).sum() ##Sum again based on stie and tracker snp\n",
    "df_merged_countv=df_merged_countv.groupby(['SITE','Sequence Name','TrackerSNP','SNP']).sum() ##Sum again based on site, tracker, and snp\n",
    "df_merged_countv = df_merged_countv.drop('Pools ', 1) ##drop pooled information\n",
    "\n",
    "#df_merged_countv.to_csv(\"testtest2.csv\", sep=',', index=True)\n",
    "\n",
    "#df_merged_countv.to_csv(\"testtest1.csv\", sep=',', index=True)\n",
    "\n",
    "df_merged_countv=df_merged_countv.reset_index()  ##Reset index so  that we can  use the columns\n",
    "df_merged_countv['Type'] = np.where(df_merged_countv['SNP'].str[0].str.isdigit(), \"Mutation\" , \"WildType\") ##If the first character of SNP value is digit assign mutation  else assign wildtype\n",
    "df_merged_countv.rename(columns={'PoolSize':'Number_of_samples'}, inplace=True) ##Reanme the Poolsize to number  of samples\n",
    "df_merged_countvp=df_merged_countv  ##Create another dataframe from previous dataframe this is for calculating products for VAF\n",
    "df_merged_countv=df_merged_countv.pivot(index=[\"SITE\", 'Sequence Name',\"TrackerSNP\"], columns=\"Type\", values=\"Number_of_samples\") ##pivot and align mutation and wildtype\n",
    "df_merged_countvp=df_merged_countvp.pivot(index=[\"SITE\", 'Sequence Name',\"TrackerSNP\"], columns=\"Type\", values=\"Prod\") ##pivot and align mutation and wildtype\n",
    "\n",
    "#df_merged_countvp.to_csv(\"testtest1.csv\", sep=',', index=True)\n",
    "\n",
    "\"\"\"\n",
    "To do: \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html\n",
    "This is where I am stuck. Trying using pivot to fix it\n",
    "\"\"\"\n",
    "\n",
    "df_merged_countvp=df_merged_countvp.reset_index() ##Reset indexes\n",
    "df_merged_countv=df_merged_countv.reset_index()  ##Reset indexes\n",
    "\n",
    "df_merged_countv['SNP'] = np.where(pd.isna(df_merged_countv['Mutation']), df_merged_countv['TrackerSNP'].astype(str).str[0:-1], df_merged_countv['TrackerSNP'].astype(str).str[1::]) ##Depending on codntion where mutation value is nan or not assign SNP with just first character or last character and amino acid position\n",
    "cols = list(df_merged_countv.columns)\n",
    "cols = cols[0:3] + cols[5:6] + cols[3:5]\n",
    "df_merged_countv = df_merged_countv[cols]\n",
    "df_merged_countv.rename(columns={'TrackerSNP':'G_Annotation'}, inplace=True)\n",
    "\n",
    "df_merged_countv['Prod']= df_merged_countvp['Mutation']\n",
    "\n",
    "df_merged_countv['Prod'] = df_merged_countv['Prod'].fillna(0)\n",
    "\n",
    "df_merged_countv['Mutation'] = df_merged_countv['Mutation'].fillna(0)\n",
    "\n",
    "df_merged_countv['WildType'] = df_merged_countv['WildType'].fillna(0)\n",
    "\n",
    "df_merged_countv['sum'] = df_merged_countv['Mutation'] + df_merged_countv['WildType']\n",
    "\n",
    "df_merged_countv['div'] = df_merged_countv['Prod'] / df_merged_countv['sum']\n",
    "\n",
    "df_merged_countv['div']=df_merged_countv['div'].round(2)\n",
    "\n",
    "df_merged_countv['div']=np.where(pd.isna(df_merged_countv['div']),\"\", df_merged_countv['div'].astype(str)+\"%\") \n",
    "\n",
    "df_merged_countv = df_merged_countv.drop('Prod', 1) ##drop pooled information\n",
    "df_merged_countv = df_merged_countv.drop('sum', 1) ##drop pooled information\n",
    "\n",
    "df_merged_countv.rename(columns={'div':'VAF'}, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "The next part requires assigning wildtype to irregular mutations which has different G_annotation\n",
    "For this part I used for loop instead of dataframe\n",
    "\"\"\"\n",
    "\n",
    "currentwild=\"\"\n",
    "currentwildnum=\"\"\n",
    "dictwild={}\n",
    "\n",
    "for index, row in df_merged_countv.iterrows():\n",
    "    if row['WildType']!=0:\n",
    "        dictwild[row[\"SITE\"],row['G_Annotation'][0:-1]]=row['WildType']\n",
    "\n",
    "    \n",
    "\n",
    "for index, row in df_merged_countv.iterrows():\n",
    "    if (row[\"SITE\"],row['G_Annotation'][0:-1]) in dictwild and row['WildType']==0:\n",
    "        #print(row['WildType'])\n",
    "        df_merged_countv.at[index,'WildType'] = dictwild[row[\"SITE\"],row['G_Annotation'][0:-1]] - row[\"Mutation\"]\n",
    "        df_merged_countv.at[index,'VAF'] = str(round((float(row['Mutation'])*float(row['VAF'][0:-1]))/(float(row['Mutation'])+float(dictwild[row[\"SITE\"],row['G_Annotation'][0:-1]]- row[\"Mutation\"])),2))+\"%\"\n",
    "\n",
    "df_merged_countv[\"Total\"]=df_merged_countv[\"Mutation\"]+df_merged_countv[\"WildType\"]\n",
    "\n",
    "df_merged_countv.to_csv(\"testtest4_EP.csv\", sep=',', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9217d2-e75e-430f-9343-ea5698d4c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_poolsize[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
