{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3fd578-a76f-448e-b881-35e663f875d2",
   "metadata": {},
   "source": [
    "## Geneious analysis for individual samples from  Geneious output, \"Annotation.csv\" ##\n",
    "\n",
    "## Required packages\n",
    " - Pandas\n",
    " - Numpy\n",
    " - Seaborn\n",
    " - matplotlib\n",
    " \n",
    "## Inputs \n",
    "    - Geneious SNP analysis of six drug resistance markers (Pfk13, Pfcrt, Pfmdr1, Pfdhfr, Pfdhps, and Pfcytb) in csv file\n",
    "    - Variants of interest file \n",
    "    \n",
    "####  * More details on how to run the Jupyter code can be found in Readme file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4068dd9d-d0e5-4a75-8a3d-f65c37a40881",
   "metadata": {},
   "source": [
    "### The below code section will output the csv file **\"*_individual_EPI_VOI.csv\"** which reports all the SNPs of interest, as defined by the VariantsOfInterest.csv file, that were called by the Geneious workflow for individually sequenced samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6265d-b51d-45a2-b3b5-ae6da76dfbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd           ## Import Pandas library for processing dataframe as pd\n",
    "import numpy as np            ## Import Numy for processing matrix as np\n",
    "import sys                    ## Import sys library for maximizing the csv limit for large csv file which can be Geneious file.\n",
    "import csv                    ## Import csv module to input the csv file to dataframe \n",
    "\n",
    "csv.field_size_limit(sys.maxsize)                                                                                                         ##Maximize the csv file input size\n",
    "Geneious_raw_DF=pd.read_csv(\"Annotations.csv\", engine='python', error_bad_lines=False)                                                    ##Import raw Geneious output table for variant analysis genes. Set up engine equals python and error_bad_lines for preventing errors with processing large csv files\n",
    "Geneious_DF_filtered_poly=Geneious_raw_DF[(Geneious_raw_DF['Type']=='Polymorphism') & (Geneious_raw_DF['Amino Acid Change'].notnull())]   ##If the type column contains polymorphism and Amino Acid Change column is not empty then create a dataframe satisfying those conditions\n",
    "Geneious_DF_filtered_cov=Geneious_raw_DF[Geneious_raw_DF['Type']=='Coverage - High']                                                      ##If the Coverage - High is in the type column as value then select dataframe for those column values\n",
    "Geneious_DF_filtered_poly[\"TrackerSNP\"]=(Geneious_DF_filtered_poly[\"Amino Acid Change\"].astype(str).str[0]+                               ##Create a TrackerSNP Column which has both amino acid before the change\n",
    "Geneious_DF_filtered_poly[\"CDS Codon Number\"].astype(int).astype(str)+                                                                    ##Create a TrackerSNP Column which has CDC Codon number for amino acid change location\n",
    "Geneious_DF_filtered_poly[\"Amino Acid Change\"].astype(str).str[-1])                                                                       ##Create a TrackerSNP Column which has both amino acid after the change\n",
    "Pre_Combined_Variant_Wildtype = [Geneious_DF_filtered_poly, Geneious_DF_filtered_cov]                                                     ##Produce a complete dataframe which contains both variants and wildtypes\n",
    "Combined_Vi_Wi = pd.concat(Pre_Combined_Variant_Wildtype)                                                                                 ##Concatenate the dataframes for variants and wildtypes\n",
    "Combination_filtered_dup=Combined_Vi_Wi.drop_duplicates(subset =[\"Document Name\", \"TrackerSNP\"] )                                         ##Drop duplicates meaning if the values are already in variants then drop it from the wildtypes\n",
    "                                                                                                                                          ##The dataframe contains information, \"Sample,POOLED,Year,SITE,TreatmentDay,GENE,G_ANNOTATION,COVERAGE,VAF,VF,SNP,TYPE\\n\")\n",
    "\n",
    "def site(row):                                ##Set up a function for assignging site based on the values in the document name column. This will change with each study please update the sites. \n",
    "    if row['Document Name'][4:6]==\"Am\":       ##Assign Amhara for the abbreviation\n",
    "        return 'Amhara'\n",
    "\n",
    "\n",
    "def POOLEDsize(row):                                          ##Set up a function for assigning TreatmentDay based on the values in the document name column\n",
    "    if row['Document Name'].replace(\" \",\"\")[13:14]==\"0\":      ##If 0 in the position 13 of the string\n",
    "        return row['Document Name'].replace(\" \",\"\")[14:15]    ##then assign only position 14 as POOLED size\n",
    "    elif row['Document Name'].replace(\" \",\"\")[13:14]!=\"0\":    ##If 0 not in the position 13 of the string\n",
    "        return row['Document Name'].replace(\" \",\"\")[13:15]    ##then assign the string from 13 and 14\n",
    "\n",
    "def TreatmentDay(row): ##Set up a function for assigning TreatmentDay based on the values in the document name column\n",
    "    if row['Document Name'][6:8]==\"00\":\n",
    "        return '0'\n",
    "    elif row['Document Name'][6:8]==\"1A\": \n",
    "        return '1'\n",
    "    elif row['Document Name'][6:8]!=\"00\" and row['Document Name'][6:8]!=\"1A\":\n",
    "        return row['Document Name'][6:8]   \n",
    "\n",
    "def POOLED(row):                                              ##Set up a function for POOLED based on the values in the document name column\n",
    "    if row['Document Name'].replace(\" \",\"\")[12:13]==\"p\":      ##If position 12 has p in it\n",
    "        return 'POOLED'                                       ##then it is considered POOLED sample\n",
    "    elif row['Document Name'].replace(\" \",\"\")[12:13]!=\"p\":    ##If position 12 has no p in it\n",
    "        return 'individual'                                   ##then it is considered individual sample\n",
    "\n",
    "def year(row):  ##Set up a function for Year  based on the values in the document name column\n",
    "    return row['Document Name'][0:2]\n",
    "\n",
    "def type_SNP(row):  ##Set up a TYPE column based on given value in the Type whether it is mutation or wildtype\n",
    "    if row['Type'] =='Polymorphism':  ##Type is either mutation or wildtype\n",
    "        return \"mutation\"\n",
    "    if row['Type'] =='Coverage - High':\n",
    "        return \"wildtype\"\n",
    "    \n",
    "def SNP(row):  ##Set up a SNP column to give pre or post amino acid changes based on mutation or wildtype\n",
    "    if row['Type'] =='Polymorphism':  ##Set up SNP notation for wildtype or mutation\n",
    "        return row['TrackerSNP'][1::]\n",
    "    if row['Type'] =='Coverage - High':\n",
    "        return row['TrackerSNP'][0:-1]\n",
    "    \n",
    "def name(row):\n",
    "    return row['Document Name'].split(\"_\")[0].replace(\" \",\"\") ##Split the document name with _ and assign first substring to clean up the name of the sample\n",
    "  \n",
    "##Apply the functions defined previously for assigning site,TreatmentDay,POOLED,year,type,SNP,name\n",
    "Combination_filtered_dup[[\"SITE\",'TREATMENT_DAY', \"POOLED\",\"YEAR\",\"TYPE\",\"SNP\",\"Document Name\" ,\"POOLEDSIZE\"]] = Combination_filtered_dup.apply([site,TreatmentDay,POOLED,year,type_SNP,SNP,name,POOLEDsize], axis=1)\n",
    "\n",
    "Combination_report1=Combination_filtered_dup[Combination_filtered_dup['Type']=='Polymorphism'] ##Select columns with mutations\n",
    "Combination_report2=Combination_filtered_dup[Combination_filtered_dup['Type']=='Coverage - High'] ##Select columns with wildtypes\n",
    "final_report1=Combination_report1[[\"Document Name\",\"Sequence Name\",\"SITE\",'TREATMENT_DAY',\"POOLED\",\"YEAR\",\"Coverage\",\"Variant Frequency\",\"Variant Raw Frequency\",\"TrackerSNP\",\"TYPE\",\"SNP\",\"POOLEDSIZE\"]] ##Assign sample information to samples with mutation\n",
    "final_report2=Combination_report2[[\"Document Name\",\"Sequence Name\",\"SITE\",'TREATMENT_DAY',\"POOLED\",\"YEAR\",\"Average Coverage\",\"Variant Frequency\",\"Variant Raw Frequency\",\"TrackerSNP\",\"TYPE\",\"SNP\",\"POOLEDSIZE\"]] ##Assign sample information to samples with wildtypes\n",
    "final_report2_re=final_report2.rename(columns={'Average Coverage': 'Coverage'}) ##Change the name of average coverage to coverage for samples with wildtypes\n",
    "final_combine=[final_report1, final_report2_re] ##Combine the information from wildtypes and mutations into one dataframe\n",
    "final_combine_2=pd.concat(final_combine) ##concatenate\n",
    "\n",
    "###This part is for polishing lab version of individual\n",
    "final_combine_4=final_combine_2[final_combine_2[\"POOLED\"]==\"individual\"] \n",
    "\n",
    "##create another dataframe where POOLED column is just individual\n",
    "#replace the rename part with one line code using dictonary\n",
    "final_combine_4=final_combine_4.rename(columns={'Document Name': 'ET_ID_IND','Sequence Name': 'GENE','TrackerSNP': 'G_ANNOTATION', 'Coverage': 'COVERAGE','Variant Frequency': 'VAF','Variant Raw Frequency': 'VF' })\n",
    "\n",
    "\n",
    "cols = list(final_combine_4.columns) ##Change columns into list\n",
    "a, b = cols.index('SNP'), cols.index('TYPE') ##Index the column to swtich the order to match the previous Guinea report\n",
    "cols[b], cols[a] = cols[a], cols[b] ##Change order of the column\n",
    "final_combine_4 = final_combine_4[cols] ##After reordering the columns reassign to the dataframe\n",
    "#fill the empty value on multiple columns using dict\n",
    "values = {\"VAF\": \"0%\", \"VF\": \"0\"}\n",
    "final_combine_4 = final_combine_4.fillna(value=values)\n",
    "\n",
    "final_combine_4=final_combine_4.replace({'Pf': 1})\n",
    "final_combine_4=final_combine_4.replace({'xx': 1})\n",
    "final_combine_4=final_combine_4.drop('POOLEDSIZE', 1)\n",
    "#final_combine_4=final_combine_4.replace({'DHPS ': 'DHPS'})\n",
    "\n",
    "#final_combine_4=final_combine_4[final_combine_4['SITE'].notnull()] ##Check if the site is empty or not and assign site \n",
    "#final_combine_4.to_csv(\"ET_individual_EPI.csv\", sep=',', index=False) ##Create a file with the dataframe for testing purpose this is internal lab version\n",
    "\n",
    "df_voi=pd.read_csv(\"VariantsOfInterest.csv\")\n",
    "df_voi[\"VOI\"]=df_voi[\"RefAA\"]+df_voi[\"AAPos\"].astype(str)+df_voi[\"AltAA\"]\n",
    "voi_list=df_voi[\"VOI\"].tolist()\n",
    "\n",
    "final_combine_4_voi=final_combine_4[final_combine_4.G_ANNOTATION.isin(voi_list)]\n",
    "#print(final_combine_4_voi)\n",
    "final_combine_4_voi.to_csv(\"ET_individual_EPI_VOI.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec51c61-0be2-4e85-a423-9c21f05fa816",
   "metadata": {},
   "source": [
    "### The below code section will output the csv file **\"*_POOLED_EPI_VOI.csv\"** which reports all the SNPs of interest, as defined by the VariantsOfInterest.csv file, that were called by the Geneious workflow for Pooled samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb1ef0-8b1c-41ea-9909-9719ff52e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POOLED_part1=pd.read_csv(\"POOLED_Info.csv\") ##import a file with information about POOLED samples\n",
    "\n",
    "#final_combine_2.rename(columns={'Document Name':'AMD ID (POOLED)'}, inplace=True) ##Reanme the document name to AET_ID\n",
    "\n",
    "\n",
    "df_merged_poolsize = final_combine_2#[final_combine_2[\"POOLED\"]==\"POOLED\"] ##merge the information based on AET_ID to add POOLED columns and POOLED size columns\n",
    "\n",
    "#df_merged_poolsize = df_merged_poolsize.drop(['SITE ','Year_y'], axis=1) ##Get rid of duplicate columns which is SITE and year_y\n",
    "\n",
    "#df_merged_poolsize.PoolSize.fillna(value=1, inplace=True) ##Fill empty values for poolsize for individual with 1s\n",
    "\n",
    "df_merged_poolsize['Variant Frequency'] = df_merged_poolsize['Variant Frequency'].fillna(\"0%\") ##Fill the empty Variant Frequency value with 0\n",
    "df_merged_poolsize['Variant Raw Frequency'] = df_merged_poolsize['Variant Raw Frequency'].fillna(0) ##Fill empty values with 0s\n",
    "#print(df_merged_poolsize)\n",
    "#df_merged_poolsize[\"Prod\"]=df_merged_poolsize[\"Variant Frequency\"].str.split('%').str[0].str.strip(\"%\").astype(float)*df_merged_poolsize[\"POOLEDsize\"].astype(float) ##Get a product by multiplying variant frequency times the pool size\n",
    "df_merged_poolsize.rename(columns={'Variant Frequency':'VAF', 'Variant Raw Frequency':'VF'}, inplace=True) ##Reanme the document name to AET_ID\n",
    "df_merged_poolsize['Coverage'] = df_merged_poolsize['Coverage'].apply(str) ##Change values from numeric to string\n",
    "df_merged_poolsize['VAF'] = df_merged_poolsize['VAF'].apply(str) ##Change values from numeric to string\n",
    "df_merged_poolsize['VF'] = df_merged_poolsize['VF'].apply(str)  ##Change values from numeric to string\n",
    "df_merged_poolsize.Coverage= df_merged_poolsize.Coverage.str.split(\" \").str[0]  ##Split the string by space for values with -> then pick the substring before the first space\n",
    "df_merged_poolsize.VAF= df_merged_poolsize.VAF.str.split(\" \").str[0] ##Split the string by space for values with -> then pick the substring before the first space\n",
    "df_merged_poolsize.VF= df_merged_poolsize.VF.str.split(\" \").str[0] ##Split the string by space for values with -> then pick the substring before the first space\n",
    "df_merged_poolsize.rename(columns={'VAF': 'Variant Frequency', 'VF':'Variant Raw Frequency'}, inplace=True) ##Reanme the document name to AET_ID\n",
    "df_merged_poolsize=df_merged_poolsize.rename(columns={'Document Name': 'ET_ID_POOLED','Sequence Name': 'GENE','TrackerSNP': 'G_ANNOTATION', 'Coverage': 'COVERAGE','Variant Frequency': 'VAF','Variant Raw Frequency': 'VF'  })\n",
    "\n",
    "\n",
    "#df_merged_poolsize=df_merged_poolsize.replace({'DHPS_437Corrected ': 'DHPS'})  ##Rename DHPS_437Corrected values to DHPS to correct the name\n",
    "df_merged_poolsize=df_merged_poolsize.replace({'DHPS ': 'DHPS'}) \n",
    "df_merged_poolsize=df_merged_poolsize.replace({'Pf': 1})\n",
    "\n",
    "df_merged_poolsize2=df_merged_poolsize[df_merged_poolsize[\"POOLED\"]==\"POOLED\"]\n",
    "#df_merged_poolsize2 = df_merged_poolsize2.drop('Prod',  1)\n",
    "\n",
    "#df_merged_poolsize2.to_csv(\"ET_POOLED_EPI.csv\", index=False)\n",
    "\n",
    "df_merged_poolsize_ind_pool=df_merged_poolsize\n",
    "#df_merged_poolsize2 = df_merged_poolsize2.drop('Prod',  1)\n",
    "\n",
    "df_voi=pd.read_csv(\"VariantsOfInterest.csv\")\n",
    "df_voi[\"VOI\"]=df_voi[\"RefAA\"]+df_voi[\"AAPos\"].astype(str)+df_voi[\"AltAA\"]\n",
    "voi_list=df_voi[\"VOI\"].tolist()\n",
    "\n",
    "df_merged_poolsize2_voi=df_merged_poolsize2[df_merged_poolsize2.G_ANNOTATION.isin(voi_list)]\n",
    "\n",
    "df_merged_poolsize2_voi=df_merged_poolsize2_voi.replace({'Pf': 1})\n",
    "\n",
    "df_merged_poolsize2_voi.to_csv(\"ET_POOLED_EPI_VOI.csv\", index=False)\n",
    "\n",
    "#df_merged_poolsize_ind_pool.to_csv(\"ET_POOLED_EPI_QC.csv\", index=False)\n",
    "\n",
    "df_merged_poolsize_ind_pool_voi=df_merged_poolsize_ind_pool[df_merged_poolsize_ind_pool.G_ANNOTATION.isin(voi_list)]\n",
    "\n",
    "#df_merged_poolsize_ind_pool_voi.to_csv(\"ET_POOLED_EPI_QC_voi.csv\", index=False)\n",
    "\n",
    "df_merged_poolsize=df_merged_poolsize.replace({'xx': 1})\n",
    "\n",
    "df_merged_poolsize=df_merged_poolsize.replace({'Pf': 1})\n",
    "\n",
    "df_merged_poolsize[\"POOLEDSIZE\"] = pd.to_numeric(df_merged_poolsize[\"POOLEDSIZE\"])\n",
    "\n",
    "df_merged_poolsize[\"Prod\"]=df_merged_poolsize[\"VAF\"].str.split('%').str[0].str.strip(\"%\").astype(float)*df_merged_poolsize[\"POOLEDSIZE\"].astype(float) ##Get a product by multiplying variant frequency times the pool size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3b01f-4119-40f5-9931-84f4c71d77bc",
   "metadata": {},
   "source": [
    "### The below code Reports a weighted variants allele frequency(VAF) average per gene for each SNPs by site/province. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe20ed2-64e5-4d1e-a5f5-c0a4bda90f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_merged_poolsize)\n",
    "df_merged_count=df_merged_poolsize.groupby(['SITE','GENE','G_ANNOTATION', 'POOLED']).sum()  ##Sum the columns based on overlapping values on site, trackersnp, and POOLED\n",
    "#print(df_merged_count)\n",
    "df_merged_countv=df_merged_poolsize.groupby(['SITE','GENE','G_ANNOTATION', 'POOLED', 'SNP']).sum() ##Sum the columns based on agreeing values on site, trackersnp, and POOLED, and snp\n",
    "df_merged_count=df_merged_count.groupby(['SITE','GENE','G_ANNOTATION']).sum() ##Sum again based on stie and tracker snp\n",
    "df_merged_countv=df_merged_countv.groupby(['SITE','GENE','G_ANNOTATION','SNP']).sum() ##Sum again based on site, tracker, and snp\n",
    "#df_merged_countv = df_merged_countv.drop('Pools ', 1) ##drop POOLED information\n",
    "df_merged_countv=df_merged_countv.reset_index()  ##Reset index so  that we can  use the columns\n",
    "df_merged_countv['Type'] = np.where(df_merged_countv['SNP'].str[0].str.isdigit(), \"Mutation\" , \"WildType\") ##If the first character of SNP value is digit assign mutation  else assign wildtype\n",
    "#print(df_merged_countv)\n",
    "\n",
    "df_merged_countv.rename(columns={'POOLEDSIZE':'Number_of_samples'}, inplace=True) ##Reanme the Poolsize to number  of samples\n",
    "\n",
    "df_merged_countv['Tracker']=df_merged_countv['G_ANNOTATION'].str[:-1]\n",
    "df_merged_countvp=df_merged_countv  ##Create another dataframe from previous dataframe this is for calculating products for VAF\n",
    "df_merged_countv=df_merged_countv.pivot(index=[\"SITE\", 'GENE',\"G_ANNOTATION\"], columns=\"Type\", values=\"Number_of_samples\") ##pivot and align mutation and wildtype\n",
    "df_merged_countvp=df_merged_countvp.pivot(index=[\"SITE\", 'GENE',\"G_ANNOTATION\"], columns=\"Type\", values=\"Prod\") ##pivot and align mutation and wildtype\n",
    "\"\"\"\n",
    "To do: \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html\n",
    "This is where I am stuck. Trying using pivot to fix it\n",
    "\"\"\"\n",
    "\n",
    "df_merged_countvp=df_merged_countvp.reset_index() ##Reset indexes\n",
    "df_merged_countv=df_merged_countv.reset_index()  ##Reset indexes\n",
    "\n",
    "df_merged_countv['SNP'] = np.where(pd.isna(df_merged_countv['Mutation']), df_merged_countv['G_ANNOTATION'].astype(str).str[0:-1], df_merged_countv['G_ANNOTATION'].astype(str).str[1::]) ##Depending on codntion where mutation value is nan or not assign SNP with just first character or last character and amino acid position\n",
    "cols = list(df_merged_countv.columns) ##Change column to list format\n",
    "cols = cols[0:3] + cols[5:6] + cols[3:5] ##Adjust the column order by adding column as lists\n",
    "df_merged_countv = df_merged_countv[cols] ##Return the column back to dataframe\n",
    "df_merged_countv.rename(columns={'TrackerSNP':'G_ANNOTATION'}, inplace=True) ##Change column name from trackersnp to G_ANNOTATION\n",
    "\n",
    "#print(df_merged_countv)\n",
    "\n",
    "df_merged_countv['Prod']= df_merged_countvp['Mutation'] ##Copy the mutation information which is sum of all the variant frequencies for each variant\n",
    "\n",
    "df_merged_countv['Prod'] = df_merged_countv['Prod'].fillna(0) ##Fill empty values with 0s\n",
    "\n",
    "df_merged_countv['Mutation'] = df_merged_countv['Mutation'].fillna(0) ##Fill empty values with 0s\n",
    "\n",
    "df_merged_countv['WildType'] = df_merged_countv['WildType'].fillna(0) ##Fill empty values with 0s\n",
    "\n",
    "df_merged_countv[\"Total\"] = df_merged_countv['Mutation'] + df_merged_countv['WildType']                        # count total by adding mutation to Wildtype\n",
    "df_merged_countv[\"VAF\"] = (df_merged_countv['Prod']/df_merged_countv[\"Total\"]).round(2).astype(str) + '%'      # count vaf by dividing prod by total\n",
    "\n",
    "cols = list(df_merged_countv.columns)  ##turn columns in to list\n",
    "a, b = cols.index('VAF'), cols.index('Total') ##Switch reindex VAF and total columns\n",
    "cols[b], cols[a] = cols[a], cols[b]\n",
    "df_merged_countv = df_merged_countv[cols] ##Assign switched column orders\n",
    "\n",
    "df_merged_countv = df_merged_countv.drop('Prod', 1) ##drop POOLED information\n",
    "\n",
    "#df_merged_countv=df_merged_countv.replace({'DHPS_437Corrected ': \"DHPS\"})\n",
    "\n",
    "df_voi=pd.read_csv(\"VariantsOfInterest.csv\")\n",
    "df_voi[\"VOI\"]=df_voi[\"RefAA\"]+df_voi[\"AAPos\"].astype(str)+df_voi[\"AltAA\"]\n",
    "voi_list=df_voi[\"VOI\"].tolist()\n",
    "\n",
    "df_merged_countv=df_merged_countv[df_merged_countv.G_ANNOTATION.isin(voi_list)]\n",
    "\n",
    "df_merged_countv.to_csv(\"ET_weighted_bysite_EPI.csv\", sep=',', index=False)\n",
    "\n",
    "#print(df_merged_countviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772bac4-b951-41d8-92ff-d435ff6b7739",
   "metadata": {},
   "source": [
    "### The below section reports how many total number of samples for Paired and Pooled samples analyzed per site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f670584-726b-4d2d-bf40-e50783c450a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicttest={} ## Create a test dictionary to check if items in the test dictionary or not\n",
    "dicttestind={} ##Create a dictionary for individuals to not count duplicatesd\n",
    "dictcat={} ##Dictionary for assining different sites\n",
    "\n",
    "with open(\"ET_pooled_EPI_VOI.csv\", \"r\") as r1: ##Loop thourgh pooled epi file\n",
    "    for lines in r1:\n",
    "        if lines[0:2].isdigit(): #and \"xx\" not in lines.split(\",\")[0]:\n",
    "            dictcat[lines[0:6]]=\"exist\" ##Create a dictionary for different sites based on first few strings of the lines\n",
    "with open(\"ET_individual_EPI_VOI.csv\", \"r\") as r2: ##Loop through individual epi file\n",
    "    for lines in r2:\n",
    "        if lines[0:2].isdigit(): #and \"xx\" not in lines.split(\",\")[0]:\n",
    "            dictcat[lines[0:6]]=\"exist\"  ##Create a dictionary for different sites based on first few strings of the lines\n",
    "\n",
    "for items in dictcat: ##Loop through different items, sites, with in the dictionary for site\n",
    "    dicttest={} ##Create an empty dictionary and fill up the dictionary test for just pooled samples\n",
    "    with open(\"ET_pooled_EPI_VOI.csv\", \"r\") as r1: ##Open the EPI file created from step 3-2\n",
    "        for lines in r1:    \n",
    "            if items in lines:\n",
    "                if \"POOLED\" in lines:\n",
    "                        dicttest[lines.split(\",\")[0]]=lines.split(\",\")[12] ##Assign pooled size to the dictionary\n",
    "                        \n",
    "        sumpooled=0\n",
    "        countpooled=0\n",
    "        for item in dicttest:\n",
    "            countpooled+=1\n",
    "            if dicttest[item]!=\"xx\\n\":\n",
    "                #print(dicttest[item])\n",
    "                sumpooled+=float(dicttest[item].strip(\"\\n\"))\n",
    "        print(\"Number of pools\", items, countpooled) ##Print out number of pools \n",
    "        print(\"sum of pools\", items, sumpooled) ##Print out sum of pools based on pool size for each pool\n",
    "for items in dictcat:\n",
    "    dicttestind={}  ##Create an empty dictionary and fill up the dictionary test for just individual samples\n",
    "    with open(\"ET_individual_EPI_VOI.csv\", \"r\") as r2:\n",
    "        for lines in r2:\n",
    "            if items in lines:\n",
    "                if \"individual\" in lines:\n",
    "                    dicttestind[lines.split(\",\")[0]]=\"exist\"\n",
    "        countindividual=0\n",
    "        for item in dicttestind:\n",
    "            countindividual+=1\n",
    "        print(\"individual total samples\", items, countindividual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d23d8-8596-40db-be62-591f1464433c",
   "metadata": {},
   "source": [
    "### The below code plots a data visualization graph for only  Paired samples. It shows if the SNPs is reported as mutations and wildtype  by each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01f053-5808-4383-ba9c-756214f89d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"ET_individual_EPI_VOI.csv\")\n",
    "\n",
    "\n",
    "# Check dataframe is correct \n",
    "df.head()\n",
    "\n",
    "# Print number of rows and columns; confirm its longform data\n",
    "print('The number of rows, columns:', df.shape) \n",
    "print('') # add space \n",
    "\n",
    "# Count number of unique samples in data \n",
    "uniq = df['ET_ID_IND']\n",
    "print('There are', uniq.nunique(), 'samples in this dataset.')\n",
    "print('') # add space \n",
    "\n",
    "# Check if there are any Null or NAs \n",
    "\n",
    "print('The number of null or NA values in data:')\n",
    "print(df.isnull().sum())\n",
    "print('') # add space \n",
    "\n",
    "# Sort data based on CODONS to ensure shown as ascending/descending in plot \n",
    "\n",
    "LS = df['G_ANNOTATION']  # Copy the full annotated SNPs into a new list called LS \n",
    "\n",
    "codon_num = [] # create an empty list \n",
    "\n",
    "# Loop through G_ANNOTATION list (LS) and strip first and last character \n",
    "for x in LS: \n",
    "    codon_num.append(int(x[1:-1]))  # IMPORTANTLY change from string to integer to allow num sorting \n",
    "    ## TODO: make this more pythonic and use rstrip() and lstrip().left()\n",
    "\n",
    "# Add the new Codon column to the current dataframe \n",
    "df['Codon'] = codon_num\n",
    "\n",
    "# Set seaborn plot style and size (NOTE: some plots will not be affected; see seaborn docs) \n",
    "\n",
    "#sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set plot/figure size \n",
    "sns.set(rc = {'figure.figsize':(15,15)})\n",
    "\n",
    "print('Plotting all SNPs categorized as wildtype or mutant using seaborn:')\n",
    "\n",
    "# Plot as seaborn strippplot sorting by Codon # and then Gene in ascending order \n",
    "\n",
    "g= sns.stripplot(data=df.sort_values(by=['GENE','Codon'], ascending=True),\n",
    "               x=\"TYPE\", y=\"G_ANNOTATION\",\n",
    "                 hue=\"GENE\")  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc31b50-c985-4b89-ab53-c1c3ac5393b5",
   "metadata": {},
   "source": [
    "### The Below two section of code plots the bar graph for Paired and Pooled together using the data from *_weighted_bysite_EPI.csv file. \n",
    "\n",
    "### Bar graph shows the wild type, major and minor allele frequencies of all the reportable SNPs described in VariantsOfInterest.csv file. Allele frequencies are indicated on the x axis, and the variants of interest and total number of samples are listed along the y-axis. The color coding indicates the type of mutation found in the samples; blue is for wild type, green for minor allele mutation and red for major allele mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d03265-e098-4de8-9f46-f572598967ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ET_weighted_bysite_EPI.csv\")\n",
    "\n",
    "df['Type'] = [\"Wildtype: AF=0%\" if float(x.strip(\"%\"))==0 else 'Minor: AF < 50%' if 0<float(x.strip(\"%\"))<0.5 else 'Major: AF >= 50%' for x in df['VAF']]\n",
    "#print(df)\n",
    "df_viz = pd.DataFrame()\n",
    "df_type = pd.DataFrame()\n",
    "\n",
    "df_type[\"GENE\"]=df.groupby([\"GENE\",\"G_ANNOTATION\",\"Type\"]).sum().reset_index()[\"GENE\"]\n",
    "df_type[\"G_ANNOTATION\"]=df.groupby([\"GENE\",\"G_ANNOTATION\",\"Type\"]).sum().reset_index()[\"G_ANNOTATION\"]\n",
    "df_type[\"Type\"]=df.groupby([\"GENE\",\"G_ANNOTATION\",\"Type\"]).sum().reset_index()[\"Type\"]\n",
    "\n",
    "df_type[\"Mutation\"]=df.groupby([\"GENE\",\"G_ANNOTATION\",\"Type\"]).sum().reset_index()[\"Mutation\"]\n",
    "df_type[\"WildType\"]=df.groupby([\"GENE\",\"G_ANNOTATION\",\"Type\"]).sum().reset_index()[\"WildType\"]\n",
    "\n",
    "\n",
    "df_type[\"Major: AF >= 50%\"]=np.where(df_type[\"Type\"]==\"Major: AF >= 50%\", df_type[\"Mutation\"], 0)\n",
    "df_type[\"Minor: AF < 50%\"]=np.where(df_type[\"Type\"]==\"Minor: AF < 50%\", df_type[\"Mutation\"], 0)\n",
    "df_type[\"Wildtype: AF=0%\"]=df_type[\"WildType\"]\n",
    "\n",
    "\n",
    "df_type=df_type.drop(['Type'], axis=1)\n",
    "\n",
    "\n",
    "df_final=df_type.groupby([\"GENE\",\"G_ANNOTATION\"]).sum().reset_index()\n",
    "\n",
    "df_final[\"GENE_G_anno\"]=df_final[\"GENE\"]+\":\"+df_final[\"G_ANNOTATION\"]\n",
    "\n",
    "df_final=df_final.drop(['GENE','G_ANNOTATION'], axis=1)\n",
    "df_final=df_final.drop(['Mutation','WildType'], axis=1)\n",
    "\n",
    "df_final[\"Total\"]=df_final[\"Wildtype: AF=0%\"]+df_final[\"Major: AF >= 50%\"]+df_final[\"Minor: AF < 50%\"]  # \n",
    "\n",
    "\n",
    "cols = list(df_final.columns)\n",
    "a, b = cols.index('GENE_G_anno'), cols.index('Major: AF >= 50%')\n",
    "cols[b], cols[a] = cols[a], cols[b]\n",
    "df_final = df_final[cols]\n",
    "\n",
    "\n",
    "df_final[\"Major: AF >= 50%\"]=df_final[\"Major: AF >= 50%\"]/df_final[\"Total\"]\n",
    "df_final[\"Minor: AF < 50%\"]=df_final[\"Minor: AF < 50%\"]/df_final[\"Total\"]\n",
    "df_final[\"Wildtype: AF=0%\"]=df_final[\"Wildtype: AF=0%\"]/df_final[\"Total\"]\n",
    "\n",
    "df_final.to_csv(\"Tab_Table_SNP_Total1_DF.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e305b-701e-4cc3-ac93-2270f36f3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "##Used the previously created table\n",
    "df_table_SNP=pd.read_csv(\"Tab_Table_SNP_Total1_DF.csv\")\n",
    "\n",
    "df_table_SNP=df_table_SNP.sort_values(by=['GENE_G_anno'])\n",
    "\n",
    "df_table_SNP[\"index\"]=df_table_SNP.GENE_G_anno.str.split(\":\").str[1].str[1:-1]\n",
    "df_table_SNP[\"index\"]=df_table_SNP[\"index\"].astype(int)\n",
    "df_table_SNP[\"index2\"]=df_table_SNP.GENE_G_anno.str.split(\":\").str[0]\n",
    "\n",
    "df_table_SNP=df_table_SNP.sort_values([\"index\"])\n",
    "#df.groupby('A', sort=False)\n",
    "df_table_SNP=df_table_SNP.groupby(['index2'], sort=False)\n",
    "df_table_SNP=pd.concat(map(lambda x: x[1], df_table_SNP))\n",
    "\n",
    "\n",
    "df_table_SNP[\"Total\"]=df_table_SNP[\"Total\"].astype(str)\n",
    "df_table_SNP[\"NewCol\"]=df_table_SNP[\"GENE_G_anno\"]+\":\"+\"N=\"+df_table_SNP[\"Total\"] ##Create the label for y-axis \n",
    "###The visualization works by adding different number of bars \n",
    "###There are three bars (total1 which has all major, minor, wildtypes, total2, just wildtype and major, and just wildtype\n",
    "###The purpose is to create a stacked bar plots\n",
    "df_table_SNP[\"total\"]=df_table_SNP[\"Minor: AF < 50%\"]+df_table_SNP[\"Major: AF >= 50%\"]+df_table_SNP[\"Wildtype: AF=0%\"] ##Create total for adding up all the bars by adding all the values for major minor and wildtype \n",
    "df_table_SNP[\"total2\"]=df_table_SNP[\"Major: AF >= 50%\"]+df_table_SNP[\"Wildtype: AF=0%\"]\n",
    "f, ax = plt.subplots(figsize = (9,11))\n",
    "sns.set_color_codes('pastel')\n",
    "##Once dataframes are created just plot the dataframe to the graph\n",
    "ax = sns.barplot(x=\"total\", y=\"NewCol\", label = \"Minor: AF < 50%\", data=df_table_SNP, color = 'r', edgecolor = 'w')\n",
    "ax = sns.barplot(x=\"total2\", y=\"NewCol\", label= \"Major: AF >= 50%\", data=df_table_SNP, color = 'g', edgecolor = 'w')\n",
    "sns.set_color_codes('deep')\n",
    "ax = sns.barplot(x=\"Wildtype: AF=0%\", y=\"NewCol\", label= \"Wildtype: AF=0%\", data=df_table_SNP, color = 'b', edgecolor = 'w')\n",
    "ax.legend(ncol = 2, loc = 'lower right')\n",
    "sns.despine(left = True, bottom = True)\n",
    "ax.set(ylabel=\"Gene+SNP+#_of_Samples\")\n",
    "ax.set(xlabel=\"Types of mutations ratio\")\n",
    "plt.legend(loc=(1.04,0))\n",
    "ax.set(title=\"Drug Resistance SNPs for Ethiopia  \", ylabel=\"SNPs\", xlabel=\"SNP ratio\")\n",
    "\n",
    "plt.savefig('ET_Bar_plot_Combined3', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4abe1e-9eb7-460b-9793-539e3126a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8976a8d2-80d6-4ea6-bf27-71fd93a3b763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6dc57b-40ed-4a76-a691-b2bf8b479c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
